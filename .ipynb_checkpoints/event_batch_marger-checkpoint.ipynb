{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a677290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] dfo_severity: 652 non-integer values found, applying round before casting.\n",
      "Merged 19 files.\n",
      "Rows written: 3219  (removed 0 duplicates).\n",
      "Unique cities: 2885.\n",
      "Output file: D:\\Development\\RESEARCH\\urban_flood_database\\event_datatables\\urban_flood_database.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- user inputs ---\n",
    "INPUT_DIR = r\"D:\\Development\\RESEARCH\\urban_flood_database\\event_datatables\"\n",
    "OUTPUT_CSV = os.path.join(INPUT_DIR, \"urban_flood_database.csv\")\n",
    "\n",
    "# Columns expected from the raw CSVs (exported from EE)\n",
    "FINAL_FIELDS = [\n",
    "    'event_id',                  # will be renamed to gfd_event_id\n",
    "    'time_start_utc',\n",
    "    'time_end_utc',\n",
    "    'duration_days',\n",
    "    'countries',\n",
    "    'dfo_main_cause',\n",
    "    'dfo_severity',\n",
    "    'dfo_dead',\n",
    "    'dfo_displaced',\n",
    "    'dfo_validation_type',\n",
    "    'composite_type',\n",
    "    'ghsl_year_used',\n",
    "    'urban_threshold_frac',\n",
    "    'urban_area_m2',\n",
    "    'urban_flood_area_m2',\n",
    "    'flood_share_urban_pct',\n",
    "    'cluster_centroid_xy_round'\n",
    "]\n",
    "\n",
    "NUMERIC_FIELDS = [\n",
    "    'duration_days',\n",
    "    'dfo_dead',\n",
    "    'dfo_displaced',\n",
    "    'ghsl_year_used',\n",
    "    'urban_threshold_frac',\n",
    "    'urban_area_m2',\n",
    "    'urban_flood_area_m2',\n",
    "    'flood_share_urban_pct'\n",
    "]\n",
    "\n",
    "def read_csv_safely(path: str) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV, align columns to FINAL_FIELDS, trim, and coerce numerics.\"\"\"\n",
    "    df = pd.read_csv(path, dtype=str, encoding=\"utf-8\", low_memory=False)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "    missing = [c for c in FINAL_FIELDS if c not in df.columns]\n",
    "    for c in missing:\n",
    "        df[c] = pd.NA\n",
    "    df = df[FINAL_FIELDS]\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            df[c] = df[c].str.strip()\n",
    "    for c in NUMERIC_FIELDS:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "# Normalization map for dfo_main_cause\n",
    "_CANON_MAP = {\n",
    "    'heavy rain': 'Heavy rain',\n",
    "    'torrential rain': 'Heavy rain',\n",
    "    'rain': 'Heavy rain',\n",
    "    'monsoonal rain': 'Monsoonal rain',\n",
    "    'monsoon rain': 'Monsoonal rain',\n",
    "    'monsoon rains': 'Monsoonal rain',\n",
    "    'monsoonal rains': 'Monsoonal rain',\n",
    "    'tropical cyclone': 'Tropical cyclone',\n",
    "    'snowmelt': 'Snowmelt',\n",
    "    'rain and snowmelt': 'Rain and snowmelt',\n",
    "    'heavy rain and snowmelt': 'Rain and snowmelt',\n",
    "}\n",
    "\n",
    "def _normalize_main_cause(val: str) -> str:\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    s = re.sub(r'\\s+', ' ', str(val).strip().lower())\n",
    "    return _CANON_MAP.get(s, s.title())\n",
    "\n",
    "def _numeric_sort_key(path: str) -> int:\n",
    "    \"\"\"Ensure natural ordering of files like ..._1.csv, ..._2.csv, ..._10.csv.\"\"\"\n",
    "    m = re.search(r\"(\\d+)\\.csv$\", path)\n",
    "    return int(m.group(1)) if m else 0\n",
    "\n",
    "def _to_int_nullable(series: pd.Series, name: str, how: str = \"round\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Coerce to numeric, then round or floor or ceil to whole numbers before casting to Int64.\n",
    "    Prints a warning if non-integers were adjusted.\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(series, errors='coerce')\n",
    "    non_int_mask = s.dropna() % 1 != 0\n",
    "    non_int_count = int(non_int_mask.sum())\n",
    "    if non_int_count > 0:\n",
    "        print(f\"[warn] {name}: {non_int_count} non-integer values found, applying {how} before casting.\")\n",
    "        if how == \"floor\":\n",
    "            s = np.floor(s)\n",
    "        elif how == \"ceil\":\n",
    "            s = np.ceil(s)\n",
    "        else:\n",
    "            s = s.round(0)\n",
    "    return s.astype('Int64')\n",
    "\n",
    "def merge_batch(input_dir: str, output_csv: str) -> None:\n",
    "    \"\"\"\n",
    "    Merge files, normalize categories, drop duplicates, assign city_id by first-ever appearance,\n",
    "    convert units, cast integer-like fields safely, create a new sequential event_id per row, and save.\n",
    "    Keeps the original time_start_utc string format. Retains cluster_centroid_xy_round in the output.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(input_dir, \"urban_flood_clusters_*.csv\")\n",
    "    csv_paths = sorted(glob.glob(pattern), key=_numeric_sort_key)\n",
    "    if not csv_paths:\n",
    "        raise FileNotFoundError(f\"No CSV files found matching pattern:\\n{pattern}\")\n",
    "\n",
    "    frames = [read_csv_safely(p) for p in csv_paths]\n",
    "    merged = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    # Normalize main cause before de-dup\n",
    "    merged['dfo_main_cause'] = merged['dfo_main_cause'].apply(_normalize_main_cause)\n",
    "\n",
    "    # Drop exact duplicates on raw EE fields\n",
    "    before = len(merged)\n",
    "    merged = merged.drop_duplicates(subset=FINAL_FIELDS)\n",
    "    after = len(merged)\n",
    "\n",
    "    # Helper datetime for stable sorting only — do not overwrite original string column\n",
    "    merged['_t_start'] = pd.to_datetime(merged['time_start_utc'], errors='coerce')\n",
    "\n",
    "    # Preserve original GFD ID as integer-like and rename the column\n",
    "    merged = merged.rename(columns={'event_id': 'gfd_event_id'})\n",
    "    merged['gfd_event_id'] = _to_int_nullable(merged['gfd_event_id'], 'gfd_event_id', how=\"round\")\n",
    "\n",
    "    # Stable chronological sort for the rows using helper column\n",
    "    merged = merged.sort_values(by=[\"_t_start\", \"gfd_event_id\"], kind=\"stable\").reset_index(drop=True)\n",
    "\n",
    "    # Assign city_id by the first time each centroid ever appears in the whole dataset\n",
    "    first_order = (\n",
    "        merged[['cluster_centroid_xy_round', '_t_start', 'gfd_event_id']]\n",
    "        .dropna(subset=['cluster_centroid_xy_round'])\n",
    "        .sort_values(by=['_t_start', 'gfd_event_id'], kind='stable')\n",
    "        .drop_duplicates(subset='cluster_centroid_xy_round', keep='first')\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    city_map = {\n",
    "        centroid: i + 1\n",
    "        for i, centroid in enumerate(first_order['cluster_centroid_xy_round'].tolist())\n",
    "    }\n",
    "    merged['city_id'] = merged['cluster_centroid_xy_round'].map(city_map).astype('Int64')\n",
    "\n",
    "    # Convert areas to km^2 and round to 2 decimals\n",
    "    merged['urban_area_km2'] = (merged['urban_area_m2'] / 1e6).round(2)\n",
    "    merged['urban_flood_area_km2'] = (merged['urban_flood_area_m2'] / 1e6).round(2)\n",
    "\n",
    "    # Round percentage to 2 decimals\n",
    "    merged['flood_share_urban_pct'] = merged['flood_share_urban_pct'].round(2)\n",
    "\n",
    "    # Cast selected integer-like fields safely\n",
    "    merged['dfo_severity'] = _to_int_nullable(merged['dfo_severity'], 'dfo_severity', how=\"round\")\n",
    "    merged['dfo_dead']     = _to_int_nullable(merged['dfo_dead'], 'dfo_dead', how=\"round\")\n",
    "\n",
    "    # Do not overwrite time_start_utc; keep original formatting from source\n",
    "    # Drop helper and raw m² columns; KEEP centroid column\n",
    "    merged = merged.drop(columns=['_t_start', 'urban_area_m2', 'urban_flood_area_m2'])\n",
    "\n",
    "    # Create new sequential event_id 1..N per row after sorting\n",
    "    merged['event_id'] = pd.RangeIndex(start=1, stop=len(merged) + 1, step=1)\n",
    "\n",
    "    # Final column order — centroid retained at the end for mapping\n",
    "    COL_ORDER = [\n",
    "        'event_id',                      # new sequential row id\n",
    "        'gfd_event_id',                  # original GFD image id as Int64\n",
    "        'city_id',\n",
    "        'time_start_utc',\n",
    "        'time_end_utc',\n",
    "        'duration_days',\n",
    "        'countries',\n",
    "        'dfo_main_cause',\n",
    "        'dfo_severity',\n",
    "        'dfo_dead',\n",
    "        'dfo_displaced',\n",
    "        'dfo_validation_type',\n",
    "        'composite_type',\n",
    "        'ghsl_year_used',\n",
    "        'urban_threshold_frac',\n",
    "        'urban_area_km2',\n",
    "        'urban_flood_area_km2',\n",
    "        'flood_share_urban_pct',\n",
    "        'cluster_centroid_xy_round',     # kept for later GEE mapping\n",
    "    ]\n",
    "    for c in COL_ORDER:\n",
    "        if c not in merged.columns:\n",
    "            merged[c] = pd.NA\n",
    "    merged = merged[COL_ORDER]\n",
    "\n",
    "    # Save\n",
    "    merged.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Merged {len(csv_paths)} files.\")\n",
    "    print(f\"Rows written: {after}  (removed {before - after} duplicates).\")\n",
    "    print(f\"Unique cities: {len(city_map)}.\")\n",
    "    print(f\"Output file: {output_csv}\")\n",
    "\n",
    "# Run\n",
    "merge_batch(INPUT_DIR, OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d85cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
