{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bca6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth Engine initialized.\n",
      "Loading full dataset: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\hydromerit_pluvial_outputs\\chronicle_df_with_pfdi_FULL.pkl\n",
      "Total events in input: 882957\n",
      "Events valid for IMERG (post-2000): 882661\n",
      "--- PREPARING WORK PLAN ---\n",
      "Found 563 existing batch files. Scanning for processed IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing existing data:  81%|█████████████████████████████████████████████▏          | 454/563 [00:23<00:03, 30.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping corrupted file D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0451.pkl: Ran out of input\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing existing data: 100%|████████████████████████████████████████████████████████| 563/563 [00:27<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events already processed: 521500\n",
      "Events remaining to process: 361161\n",
      "Plan: Processing 350 batches.\n",
      "\n",
      "Processing Batch 563 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 563:   0%|                                                                              | 0/1000 [00:00<?, ?it/s]C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\ee\\deprecation.py:214: DeprecationWarning: \n",
      "\n",
      "Attention required for NASA/GPM_L3/IMERG_V06! You are using a deprecated asset.\n",
      "To make sure your code keeps working, please update it.\n",
      "This dataset has been superseded by NASA/GPM_L3/IMERG_V07\n",
      "\n",
      "Learn more: https://developers.google.com/earth-engine/datasets/catalog/NASA_GPM_L3_IMERG_V06\n",
      "\n",
      "  warnings.warn(warning, category=DeprecationWarning)\n",
      "Batch 563: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:20<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0563.pkl\n",
      "\n",
      "Processing Batch 564 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 564: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [09:54<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0564.pkl\n",
      "\n",
      "Processing Batch 565 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 565: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:10<00:00,  1.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0565.pkl\n",
      "\n",
      "Processing Batch 566 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 566: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:13<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0566.pkl\n",
      "\n",
      "Processing Batch 567 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 567: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [11:50<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0567.pkl\n",
      "\n",
      "Processing Batch 568 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 568: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:14<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0568.pkl\n",
      "\n",
      "Processing Batch 569 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 569: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [09:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0569.pkl\n",
      "\n",
      "Processing Batch 570 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 570: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:20<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0570.pkl\n",
      "\n",
      "Processing Batch 571 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 571: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:21<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0571.pkl\n",
      "\n",
      "Processing Batch 572 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 572: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:34<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0572.pkl\n",
      "\n",
      "Processing Batch 573 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 573: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [32:54<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0573.pkl\n",
      "\n",
      "Processing Batch 574 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 574: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [13:11<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0574.pkl\n",
      "\n",
      "Processing Batch 575 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 575: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [11:41<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0575.pkl\n",
      "\n",
      "Processing Batch 576 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 576: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:03<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0576.pkl\n",
      "\n",
      "Processing Batch 577 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 577: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:32<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0577.pkl\n",
      "\n",
      "Processing Batch 578 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 578: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:22<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0578.pkl\n",
      "\n",
      "Processing Batch 579 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 579: 100%|███████████████████████████████████████████████████████████████████| 1000/1000 [10:14<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0579.pkl\n",
      "\n",
      "Processing Batch 580 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 580:  55%|█████████████████████████████████████                               | 545/1000 [05:42<04:06,  1.85it/s]"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0) IMPORTS\n",
    "# ============================================\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely.wkt\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "import rasterio.features\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import gc  # Garbage Collector for memory management\n",
    "\n",
    "import ee\n",
    "\n",
    "# ============================================\n",
    "# 1) CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# --- INPUT: The consolidated PKL from step 02a ---\n",
    "INPUT_PKL = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\hydromerit_pluvial_outputs\\chronicle_df_with_pfdi_FULL.pkl\"\n",
    "\n",
    "# --- OUTPUT: Where rain data will be saved ---\n",
    "OUT_DIR = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\"\n",
    "OUT_FINAL_PKL = os.path.join(OUT_DIR, \"chronicle_urban_df_with_IMERG_FULL.pkl\")\n",
    "\n",
    "# IMERG Constants\n",
    "# Data available from June 2000\n",
    "IMERG_START_DATE = pd.Timestamp(\"2000-06-01\") \n",
    "SCALE = 0.1  # 0.1 Degree resolution\n",
    "CRS = 'EPSG:4326'\n",
    "\n",
    "# Batch Settings\n",
    "# Keeping it safe at 100 to avoid memory overflow with 3D arrays\n",
    "BATCH_SIZE = 1000 \n",
    "N_BATCHES_TO_RUN = 350  # Limit execution to 30 batches (3000 events)\n",
    "\n",
    "# ============================================\n",
    "# 2) HELPERS\n",
    "# ============================================\n",
    "\n",
    "def ensure_out_dir(path):\n",
    "    \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def initialize_ee():\n",
    "    \"\"\"Initialize Earth Engine.\"\"\"\n",
    "    try:\n",
    "        ee.Initialize()\n",
    "        print(\"Earth Engine initialized.\")\n",
    "    except Exception:\n",
    "        print(\"Authenticating Earth Engine...\")\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize()\n",
    "        print(\"Earth Engine initialized after auth.\")\n",
    "\n",
    "def get_next_batch_index(out_dir):\n",
    "    \"\"\"\n",
    "    Scans output directory for 'imerg_batch_XXXX.pkl' to determine \n",
    "    the next batch number for file naming.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        return 0\n",
    "    \n",
    "    pattern = os.path.join(out_dir, \"imerg_batch_*.pkl\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        return 0\n",
    "    \n",
    "    max_batch = -1\n",
    "    for f in files:\n",
    "        try:\n",
    "            filename = os.path.basename(f)\n",
    "            # filename format: imerg_batch_0001.pkl\n",
    "            num_part = filename.split('_')[-1].split('.')[0]\n",
    "            batch_num = int(num_part)\n",
    "            if batch_num > max_batch:\n",
    "                max_batch = batch_num\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    return max_batch + 1\n",
    "\n",
    "def extract_rain_data(event_row):\n",
    "    \"\"\"\n",
    "    Extracts IMERG rain matrix, metadata, and mask for a single event.\n",
    "    \n",
    "    Time Window Logic:\n",
    "    - Start: 72 hours BEFORE event start.\n",
    "    - End: 24 hours AFTER event end.\n",
    "    \n",
    "    Returns: (rain_matrix, rain_mask, rain_meta)\n",
    "    \"\"\"\n",
    "    # 1. Geometry Setup\n",
    "    try:\n",
    "        # Load geometry from WKT\n",
    "        if isinstance(event_row['geometry_wkt'], str):\n",
    "            poly_geom = shapely.wkt.loads(event_row['geometry_wkt'])\n",
    "        else:\n",
    "            # Fallback if it's already a geometry object\n",
    "            poly_geom = event_row['geometry_wkt']\n",
    "            \n",
    "        bounds = poly_geom.bounds # (minx, miny, maxx, maxy)\n",
    "        roi = ee.Geometry.BBox(*bounds)\n",
    "        \n",
    "        # 2. Time Window Calculation\n",
    "        # UPDATED: Taking 72 hours prior to the start time\n",
    "        start_t = event_row['start_time'] - pd.Timedelta(hours=72)\n",
    "        # We keep a buffer after the end time as well (e.g., 24 hours)\n",
    "        end_t = event_row['end_time'] + pd.Timedelta(hours=24) \n",
    "        \n",
    "        # 3. GEE Request\n",
    "        imerg_coll = ee.ImageCollection(\"NASA/GPM_L3/IMERG_V06\") \\\n",
    "            .select('precipitationCal') \\\n",
    "            .filterBounds(roi) \\\n",
    "            .filterDate(start_t, end_t)\n",
    "        \n",
    "        # Check if collection is empty\n",
    "        if imerg_coll.size().getInfo() == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # 4. Download Data (sampleRectangle)\n",
    "        # Convert collection to a multi-band image (each band is a time step)\n",
    "        stack = imerg_coll.toBands()\n",
    "        \n",
    "        # sampleRectangle downloads the raw pixels within the BBox\n",
    "        pixel_dict = stack.sampleRectangle(region=roi).getInfo()\n",
    "        properties = pixel_dict['properties']\n",
    "        \n",
    "        # 5. Parse & Stack Arrays\n",
    "        # Keys are like '20000604120000_precipitationCal'\n",
    "        band_keys = sorted(list(properties.keys()))\n",
    "        \n",
    "        arrays_list = []\n",
    "        for b in band_keys:\n",
    "            # Convert list to numpy array (float32 to save memory)\n",
    "            arr = np.array(properties[b], dtype=np.float32)\n",
    "            arrays_list.append(arr)\n",
    "            \n",
    "        # Stack into 3D Array: (Time, Height, Width)\n",
    "        rain_matrix = np.stack(arrays_list)\n",
    "        \n",
    "        # 6. Create Metadata (Anchor)\n",
    "        height, width = rain_matrix.shape[1], rain_matrix.shape[2]\n",
    "        min_lon, min_lat, max_lon, max_lat = bounds\n",
    "        \n",
    "        # Transform for Rasterio (Lat/Lon)\n",
    "        # Note: We align to the BBox top-left\n",
    "        transform = Affine(SCALE, 0, min_lon, 0, -SCALE, max_lat)\n",
    "        \n",
    "        meta = {\n",
    "            'origin_top_left': (max_lat, min_lon), # (Lat, Lon)\n",
    "            'scale': SCALE,\n",
    "            'shape': (height, width),\n",
    "            'timestamps': band_keys # Store timestamp keys to map matrix layers to time\n",
    "        }\n",
    "\n",
    "        # 7. Create Binary Mask (Polygon shape on grid)\n",
    "        # 1 = Inside Polygon, 0 = Outside\n",
    "        mask = rasterio.features.rasterize(\n",
    "            [(poly_geom, 1)],\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            all_touched=True,\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        return rain_matrix, mask, meta\n",
    "\n",
    "    except Exception as e:\n",
    "        # If extraction fails (e.g., GEE timeout), return None\n",
    "        return None, None, None\n",
    "\n",
    "# ============================================\n",
    "# 3) INITIALIZATION\n",
    "# ============================================\n",
    "initialize_ee()\n",
    "ensure_out_dir(OUT_DIR)\n",
    "\n",
    "# Load Input Data (The consolidated file from 02a)\n",
    "print(f\"Loading full dataset: {INPUT_PKL}\")\n",
    "if not os.path.exists(INPUT_PKL):\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_PKL}. Please run 02a first.\")\n",
    "\n",
    "df = pd.read_pickle(INPUT_PKL)\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df['start_time'] = pd.to_datetime(df['start_time'], unit='s')\n",
    "df['end_time'] = pd.to_datetime(df['end_time'], unit='s')\n",
    "\n",
    "# Filter for IMERG Era (Post June 2000)\n",
    "# Events before this date will not have IMERG data\n",
    "df_valid = df[df['start_time'] >= IMERG_START_DATE].copy()\n",
    "\n",
    "print(f\"Total events in input: {len(df)}\")\n",
    "print(f\"Events valid for IMERG (post-2000): {len(df_valid)}\")\n",
    "\n",
    "# ============================================\n",
    "# 4) SMART BATCH PROCESSING (ID-BASED)\n",
    "# ============================================\n",
    "\n",
    "print(f\"--- PREPARING WORK PLAN ---\")\n",
    "\n",
    "# 1. Identify what is already done\n",
    "# We scan the output directory for existing RAIN batches to avoid re-processing\n",
    "processed_ids = set()\n",
    "pkl_pattern = os.path.join(OUT_DIR, \"imerg_batch_*.pkl\")\n",
    "existing_files = glob.glob(pkl_pattern)\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"Found {len(existing_files)} existing batch files. Scanning for processed IDs...\")\n",
    "    for f in tqdm(existing_files, desc=\"Indexing existing data\"):\n",
    "        try:\n",
    "            # Only read columns needed for ID check to save memory\n",
    "            df_temp = pd.read_pickle(f)\n",
    "            if 'event_id' in df_temp.columns:\n",
    "                processed_ids.update(df_temp['event_id'].tolist())\n",
    "            del df_temp\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupted file {f}: {e}\")\n",
    "\n",
    "print(f\"Total events already processed: {len(processed_ids)}\")\n",
    "\n",
    "# 2. Filter the Main DataFrame\n",
    "# We keep only rows whose ID is NOT in the processed set\n",
    "df_todo = df_valid[~df_valid['event_id'].isin(processed_ids)].copy()\n",
    "\n",
    "print(f\"Events remaining to process: {len(df_todo)}\")\n",
    "\n",
    "if len(df_todo) == 0:\n",
    "    print(\"All events are already processed! Nothing to do.\")\n",
    "else:\n",
    "    # 3. Process the remaining rows in new batches\n",
    "    start_batch_num = get_next_batch_index(OUT_DIR)\n",
    "    n_remaining = len(df_todo)\n",
    "    \n",
    "    # Calculate stop limit based on N_BATCHES_TO_RUN\n",
    "    max_rows_limit = N_BATCHES_TO_RUN * BATCH_SIZE\n",
    "    stop_at_row = min(n_remaining, max_rows_limit)\n",
    "\n",
    "    print(f\"Plan: Processing {min(N_BATCHES_TO_RUN, n_remaining // BATCH_SIZE + 1)} batches.\")\n",
    "    \n",
    "    # Iterate in chunks up to stop_at_row\n",
    "    for batch_i in range(0, stop_at_row, BATCH_SIZE):\n",
    "        \n",
    "        # Determine actual batch number for filename\n",
    "        current_file_num = start_batch_num + (batch_i // BATCH_SIZE)\n",
    "        \n",
    "        # Slice the TODO dataframe\n",
    "        batch_df = df_todo.iloc[batch_i : batch_i + BATCH_SIZE].copy()\n",
    "        \n",
    "        print(f\"\\nProcessing Batch {current_file_num} ({len(batch_df)} events)...\")\n",
    "        \n",
    "        matrices = []\n",
    "        masks = []\n",
    "        metas = []\n",
    "        \n",
    "        # Inner loop: iterate rows in current batch\n",
    "        for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {current_file_num}\"):\n",
    "            mat, msk, mt = extract_rain_data(row)\n",
    "            matrices.append(mat)\n",
    "            masks.append(msk)\n",
    "            metas.append(mt)\n",
    "        \n",
    "        # Assign results to columns\n",
    "        batch_df['imerg_matrix'] = matrices\n",
    "        batch_df['imerg_mask'] = masks\n",
    "        batch_df['imerg_meta'] = metas\n",
    "        \n",
    "        # Save batch to disk (Pickle)\n",
    "        out_path = os.path.join(OUT_DIR, f\"imerg_batch_{current_file_num:04d}.pkl\")\n",
    "        batch_df.to_pickle(out_path)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "        \n",
    "        # === MEMORY CLEANUP ===\n",
    "        # Explicitly delete large objects to free RAM for next iteration\n",
    "        del batch_df\n",
    "        del matrices\n",
    "        del masks\n",
    "        del metas\n",
    "        gc.collect() # Force garbage collection\n",
    "        # ======================\n",
    "\n",
    "    print(\"\\n--- BATCH LIMIT REACHED ---\")\n",
    "    print(f\"Stopped execution after {N_BATCHES_TO_RUN} batches as requested.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec630b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f59e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37881e42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fe12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d714427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5) FINAL MERGE & CLEANUP\n",
    "# ============================================\n",
    "print(\"\\n--- FINALIZING ---\")\n",
    "print(\"Merging all batch files...\")\n",
    "\n",
    "pkl_pattern = os.path.join(OUT_DIR, \"imerg_batch_*.pkl\")\n",
    "all_pkl_files = glob.glob(pkl_pattern)\n",
    "\n",
    "if not all_pkl_files:\n",
    "    print(\"No output files found.\")\n",
    "else:\n",
    "    # Concatenate all batches\n",
    "    df_list = []\n",
    "    for f in tqdm(all_pkl_files, desc=\"Loading Batches\"):\n",
    "        try:\n",
    "            df_list.append(pd.read_pickle(f))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {f}: {e}\")\n",
    "            \n",
    "    if df_list:\n",
    "        # This DataFrame contains ONLY the processed events with rain data\n",
    "        df_results = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df_results)} processed rain events (raw count).\")\n",
    "        print(\"Merging results back to main dataset structure...\")\n",
    "        \n",
    "        # Reload the original input again to ensure we have the clean base columns\n",
    "        df_base = pd.read_pickle(INPUT_PKL)\n",
    "        \n",
    "        # Merge: 'inner' keeps only keys that appear in BOTH DataFrames\n",
    "        df_final = df_base.merge(\n",
    "            df_results[['event_id', 'imerg_matrix', 'imerg_mask', 'imerg_meta']], \n",
    "            on='event_id', \n",
    "            how='inner' \n",
    "        )\n",
    "        \n",
    "        # --- NEW: FILTER OUT NULL RAIN DATA ---\n",
    "        \n",
    "        # 1. Ensure start_time is datetime format for readable printing\n",
    "        if 'start_time' in df_final.columns:\n",
    "            df_final['start_time'] = pd.to_datetime(df_final['start_time'], unit='s')\n",
    "\n",
    "        # 2. Identify rows where 'imerg_matrix' is Null/None (Values of 0 are NOT null, so they stay)\n",
    "        missing_rain_mask = df_final['imerg_matrix'].isnull()\n",
    "        missing_events = df_final[missing_rain_mask]\n",
    "        \n",
    "        # 3. Print dates of dropped events\n",
    "        if not missing_events.empty:\n",
    "            print(f\"\\n[WARNING] Found {len(missing_events)} events with NULL rain data. Removing them...\")\n",
    "            print(\"--- Dropped Events Log ---\")\n",
    "            for idx, row in missing_events.iterrows():\n",
    "                try:\n",
    "                    d_str = row['start_time'].strftime('%d-%m-%Y %H:%M')\n",
    "                except:\n",
    "                    d_str = str(row['start_time'])\n",
    "                print(f\"Removing ID: {row['event_id']} | Date: {d_str}\")\n",
    "            \n",
    "            # 4. Perform the drop\n",
    "            df_final = df_final[~missing_rain_mask].copy()\n",
    "            print(f\"--- Cleaned. Remaining events: {len(df_final)} ---\")\n",
    "        else:\n",
    "            print(\"No NULL rain events found. All processed events are valid.\")\n",
    "\n",
    "        # --------------------------------------\n",
    "\n",
    "        # Save Final PKL (Cleaned subset)\n",
    "        df_final.to_pickle(OUT_FINAL_PKL)\n",
    "        \n",
    "        print(f\"SUCCESS! Final dataset saved to: {OUT_FINAL_PKL}\")\n",
    "        \n",
    "        # Final Verification\n",
    "        if 'imerg_matrix' in df_final.columns:\n",
    "            count = df_final['imerg_matrix'].notnull().sum()\n",
    "            print(f\"Verified valid Rain Data events: {count}\")\n",
    "    else:\n",
    "        print(\"Failed to load any batch files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d17bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc5379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
