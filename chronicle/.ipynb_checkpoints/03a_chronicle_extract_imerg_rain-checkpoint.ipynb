{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d8b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earth Engine initialized.\n",
      "Loading full dataset: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\hydromerit_pluvial_outputs\\chronicle_df_with_pfdi_FULL.pkl\n",
      "Total events in input: 882957\n",
      "Events valid for IMERG (post-2000): 882661\n",
      "--- PREPARING WORK PLAN ---\n",
      "Found 70 existing batch files. Scanning for processed IDs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing existing data: 100%|██████████████████████████████████████████████████████████| 70/70 [00:01<00:00, 60.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total events already processed: 29500\n",
      "Events remaining to process: 853161\n",
      "Plan: Processing 100 batches.\n",
      "\n",
      "Processing Batch 70 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 70:   0%|                                                                               | 0/1000 [00:00<?, ?it/s]C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\ee\\deprecation.py:214: DeprecationWarning: \n",
      "\n",
      "Attention required for NASA/GPM_L3/IMERG_V06! You are using a deprecated asset.\n",
      "To make sure your code keeps working, please update it.\n",
      "This dataset has been superseded by NASA/GPM_L3/IMERG_V07\n",
      "\n",
      "Learn more: https://developers.google.com/earth-engine/datasets/catalog/NASA_GPM_L3_IMERG_V06\n",
      "\n",
      "  warnings.warn(warning, category=DeprecationWarning)\n",
      "Batch 70: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:17<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0070.pkl\n",
      "\n",
      "Processing Batch 71 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 71: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:10<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0071.pkl\n",
      "\n",
      "Processing Batch 72 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 72: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:28<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0072.pkl\n",
      "\n",
      "Processing Batch 73 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 73: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:21<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0073.pkl\n",
      "\n",
      "Processing Batch 74 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 74: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:27<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0074.pkl\n",
      "\n",
      "Processing Batch 75 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 75: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:21<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0075.pkl\n",
      "\n",
      "Processing Batch 76 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 76: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:40<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0076.pkl\n",
      "\n",
      "Processing Batch 77 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 77: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:29<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0077.pkl\n",
      "\n",
      "Processing Batch 78 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 78: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:47<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0078.pkl\n",
      "\n",
      "Processing Batch 79 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 79: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:22<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0079.pkl\n",
      "\n",
      "Processing Batch 80 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 80: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:34<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0080.pkl\n",
      "\n",
      "Processing Batch 81 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 81: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:12<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0081.pkl\n",
      "\n",
      "Processing Batch 82 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 82: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:29<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0082.pkl\n",
      "\n",
      "Processing Batch 83 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 83: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:01<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0083.pkl\n",
      "\n",
      "Processing Batch 84 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 84: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:19<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0084.pkl\n",
      "\n",
      "Processing Batch 85 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 85: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:08<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0085.pkl\n",
      "\n",
      "Processing Batch 86 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 86: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:28<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0086.pkl\n",
      "\n",
      "Processing Batch 87 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 87: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:17<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0087.pkl\n",
      "\n",
      "Processing Batch 88 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 88: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:21<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0088.pkl\n",
      "\n",
      "Processing Batch 89 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 89: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:57<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0089.pkl\n",
      "\n",
      "Processing Batch 90 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 90: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:30<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0090.pkl\n",
      "\n",
      "Processing Batch 91 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 91: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:29<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0091.pkl\n",
      "\n",
      "Processing Batch 92 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 92: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:08<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0092.pkl\n",
      "\n",
      "Processing Batch 93 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 93: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:18<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0093.pkl\n",
      "\n",
      "Processing Batch 94 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 94: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:51<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0094.pkl\n",
      "\n",
      "Processing Batch 95 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 95: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:27<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0095.pkl\n",
      "\n",
      "Processing Batch 96 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 96: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:21<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0096.pkl\n",
      "\n",
      "Processing Batch 97 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 97: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [09:26<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0097.pkl\n",
      "\n",
      "Processing Batch 98 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 98: 100%|████████████████████████████████████████████████████████████████████| 1000/1000 [10:00<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\imerg_batch_0098.pkl\n",
      "\n",
      "Processing Batch 99 (1000 events)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 99:   1%|▉                                                                     | 13/1000 [00:08<10:07,  1.62it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\raznu\\AppData\\Local\\Temp\\ipykernel_21160\\1193799961.py\", line 266, in <module>\n",
      "    mat, msk, mt = extract_rain_data(row)\n",
      "  File \"C:\\Users\\raznu\\AppData\\Local\\Temp\\ipykernel_21160\\1193799961.py\", line 123, in extract_rain_data\n",
      "    if imerg_coll.size().getInfo() == 0:\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\ee\\computedobject.py\", line 107, in getInfo\n",
      "    return data.computeValue(self)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\ee\\data.py\", line 1064, in computeValue\n",
      "    return _execute_cloud_call(\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\ee\\data.py\", line 349, in _execute_cloud_call\n",
      "    return call.execute(num_retries=num_retries)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\googleapiclient\\_helpers.py\", line 130, in positional_wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\", line 923, in execute\n",
      "    resp, content = _retry_request(\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\", line 191, in _retry_request\n",
      "    resp, content = http.request(uri, method, *args, **kwargs)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\google_auth_httplib2.py\", line 218, in request\n",
      "    response, content = self.http.request(\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\ee\\_cloud_api_utils.py\", line 78, in request\n",
      "    response = self._session.request(\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 589, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 703, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\", line 644, in send\n",
      "    resp = conn.urlopen(\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 787, in urlopen\n",
      "    response = self._make_request(\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\", line 534, in _make_request\n",
      "    response = conn.getresponse()\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\", line 571, in getresponse\n",
      "    httplib_response = super().getresponse()\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\http\\client.py\", line 1377, in getresponse\n",
      "    response.begin()\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\http\\client.py\", line 320, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\http\\client.py\", line 281, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\ssl.py\", line 1242, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\ssl.py\", line 1100, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\inspect.py\", line 1543, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\inspect.py\", line 1501, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\inspect.py\", line 709, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\inspect.py\", line 752, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\inspect.py\", line 721, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\inspect.py\", line 706, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\raznu\\Anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21160\\1193799961.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"Batch {current_file_num}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m             \u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_rain_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m             \u001b[0mmatrices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21160\\1193799961.py\u001b[0m in \u001b[0;36mextract_rain_data\u001b[1;34m(event_row)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# Check if collection is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mimerg_coll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ee\\computedobject.py\u001b[0m in \u001b[0;36mgetInfo\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomputeValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ee\\data.py\u001b[0m in \u001b[0;36mcomputeValue\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m   1063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m   return _execute_cloud_call(\n\u001b[0m\u001b[0;32m   1065\u001b[0m       \u001b[0m_get_cloud_projects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ee\\data.py\u001b[0m in \u001b[0;36m_execute_cloud_call\u001b[1;34m(call, num_retries)\u001b[0m\n\u001b[0;32m    348\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_retries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mgoogleapiclient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHttpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googleapiclient\\_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    922\u001b[0m         \u001b[1;31m# Handle retries for server-side errors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         resp, content = _retry_request(\n\u001b[0m\u001b[0;32m    924\u001b[0m             \u001b[0mhttp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\googleapiclient\\http.py\u001b[0m in \u001b[0;36m_retry_request\u001b[1;34m(http, num_retries, req_type, sleep, rand, uri, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m    190\u001b[0m             \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhttp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;31m# Retry on SSL errors and socket timeout errors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\google_auth_httplib2.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, uri, method, body, headers, redirections, connection_type, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# Make the request.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         response, content = self.http.request(\n\u001b[0m\u001b[0;32m    219\u001b[0m             \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\ee\\_cloud_api_utils.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     77\u001b[0m       \u001b[1;31m# consider transient.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       response = self._session.request(\n\u001b[0m\u001b[0;32m     79\u001b[0m           \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             resp = conn.urlopen(\n\u001b[0m\u001b[0;32m    645\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m             response = self._make_request(\n\u001b[0m\u001b[0;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1377\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 281\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    282\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 704\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    705\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1241\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1243\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1099\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2076\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 0) IMPORTS\n",
    "# ============================================\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shapely.wkt\n",
    "import shapely.geometry\n",
    "import geopandas as gpd\n",
    "from affine import Affine\n",
    "import rasterio.features\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import gc  # Garbage Collector for memory management\n",
    "\n",
    "import ee\n",
    "\n",
    "# ============================================\n",
    "# 1) CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# --- INPUT: The consolidated PKL from step 02a ---\n",
    "INPUT_PKL = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\hydromerit_pluvial_outputs\\chronicle_df_with_pfdi_FULL.pkl\"\n",
    "\n",
    "# --- OUTPUT: Where rain data will be saved ---\n",
    "OUT_DIR = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\"\n",
    "OUT_FINAL_PKL = os.path.join(OUT_DIR, \"chronicle_urban_df_with_IMERG_FULL.pkl\")\n",
    "\n",
    "# IMERG Constants\n",
    "# Data available from June 2000\n",
    "IMERG_START_DATE = pd.Timestamp(\"2000-06-01\") \n",
    "SCALE = 0.1  # 0.1 Degree resolution\n",
    "CRS = 'EPSG:4326'\n",
    "\n",
    "# Batch Settings\n",
    "# Keeping it safe at 100 to avoid memory overflow with 3D arrays\n",
    "BATCH_SIZE = 1000 \n",
    "N_BATCHES_TO_RUN = 100  # Limit execution to 30 batches (3000 events)\n",
    "\n",
    "# ============================================\n",
    "# 2) HELPERS\n",
    "# ============================================\n",
    "\n",
    "def ensure_out_dir(path):\n",
    "    \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def initialize_ee():\n",
    "    \"\"\"Initialize Earth Engine.\"\"\"\n",
    "    try:\n",
    "        ee.Initialize()\n",
    "        print(\"Earth Engine initialized.\")\n",
    "    except Exception:\n",
    "        print(\"Authenticating Earth Engine...\")\n",
    "        ee.Authenticate()\n",
    "        ee.Initialize()\n",
    "        print(\"Earth Engine initialized after auth.\")\n",
    "\n",
    "def get_next_batch_index(out_dir):\n",
    "    \"\"\"\n",
    "    Scans output directory for 'imerg_batch_XXXX.pkl' to determine \n",
    "    the next batch number for file naming.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        return 0\n",
    "    \n",
    "    pattern = os.path.join(out_dir, \"imerg_batch_*.pkl\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    if not files:\n",
    "        return 0\n",
    "    \n",
    "    max_batch = -1\n",
    "    for f in files:\n",
    "        try:\n",
    "            filename = os.path.basename(f)\n",
    "            # filename format: imerg_batch_0001.pkl\n",
    "            num_part = filename.split('_')[-1].split('.')[0]\n",
    "            batch_num = int(num_part)\n",
    "            if batch_num > max_batch:\n",
    "                max_batch = batch_num\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    return max_batch + 1\n",
    "\n",
    "def extract_rain_data(event_row):\n",
    "    \"\"\"\n",
    "    Extracts IMERG rain matrix, metadata, and mask for a single event.\n",
    "    \n",
    "    Time Window Logic:\n",
    "    - Start: 72 hours BEFORE event start.\n",
    "    - End: 24 hours AFTER event end.\n",
    "    \n",
    "    Returns: (rain_matrix, rain_mask, rain_meta)\n",
    "    \"\"\"\n",
    "    # 1. Geometry Setup\n",
    "    try:\n",
    "        # Load geometry from WKT\n",
    "        if isinstance(event_row['geometry_wkt'], str):\n",
    "            poly_geom = shapely.wkt.loads(event_row['geometry_wkt'])\n",
    "        else:\n",
    "            # Fallback if it's already a geometry object\n",
    "            poly_geom = event_row['geometry_wkt']\n",
    "            \n",
    "        bounds = poly_geom.bounds # (minx, miny, maxx, maxy)\n",
    "        roi = ee.Geometry.BBox(*bounds)\n",
    "        \n",
    "        # 2. Time Window Calculation\n",
    "        # UPDATED: Taking 72 hours prior to the start time\n",
    "        start_t = event_row['start_time'] - pd.Timedelta(hours=72)\n",
    "        # We keep a buffer after the end time as well (e.g., 24 hours)\n",
    "        end_t = event_row['end_time'] + pd.Timedelta(hours=24) \n",
    "        \n",
    "        # 3. GEE Request\n",
    "        imerg_coll = ee.ImageCollection(\"NASA/GPM_L3/IMERG_V06\") \\\n",
    "            .select('precipitationCal') \\\n",
    "            .filterBounds(roi) \\\n",
    "            .filterDate(start_t, end_t)\n",
    "        \n",
    "        # Check if collection is empty\n",
    "        if imerg_coll.size().getInfo() == 0:\n",
    "            return None, None, None\n",
    "\n",
    "        # 4. Download Data (sampleRectangle)\n",
    "        # Convert collection to a multi-band image (each band is a time step)\n",
    "        stack = imerg_coll.toBands()\n",
    "        \n",
    "        # sampleRectangle downloads the raw pixels within the BBox\n",
    "        pixel_dict = stack.sampleRectangle(region=roi).getInfo()\n",
    "        properties = pixel_dict['properties']\n",
    "        \n",
    "        # 5. Parse & Stack Arrays\n",
    "        # Keys are like '20000604120000_precipitationCal'\n",
    "        band_keys = sorted(list(properties.keys()))\n",
    "        \n",
    "        arrays_list = []\n",
    "        for b in band_keys:\n",
    "            # Convert list to numpy array (float32 to save memory)\n",
    "            arr = np.array(properties[b], dtype=np.float32)\n",
    "            arrays_list.append(arr)\n",
    "            \n",
    "        # Stack into 3D Array: (Time, Height, Width)\n",
    "        rain_matrix = np.stack(arrays_list)\n",
    "        \n",
    "        # 6. Create Metadata (Anchor)\n",
    "        height, width = rain_matrix.shape[1], rain_matrix.shape[2]\n",
    "        min_lon, min_lat, max_lon, max_lat = bounds\n",
    "        \n",
    "        # Transform for Rasterio (Lat/Lon)\n",
    "        # Note: We align to the BBox top-left\n",
    "        transform = Affine(SCALE, 0, min_lon, 0, -SCALE, max_lat)\n",
    "        \n",
    "        meta = {\n",
    "            'origin_top_left': (max_lat, min_lon), # (Lat, Lon)\n",
    "            'scale': SCALE,\n",
    "            'shape': (height, width),\n",
    "            'timestamps': band_keys # Store timestamp keys to map matrix layers to time\n",
    "        }\n",
    "\n",
    "        # 7. Create Binary Mask (Polygon shape on grid)\n",
    "        # 1 = Inside Polygon, 0 = Outside\n",
    "        mask = rasterio.features.rasterize(\n",
    "            [(poly_geom, 1)],\n",
    "            out_shape=(height, width),\n",
    "            transform=transform,\n",
    "            fill=0,\n",
    "            all_touched=True,\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        \n",
    "        return rain_matrix, mask, meta\n",
    "\n",
    "    except Exception as e:\n",
    "        # If extraction fails (e.g., GEE timeout), return None\n",
    "        return None, None, None\n",
    "\n",
    "# ============================================\n",
    "# 3) INITIALIZATION\n",
    "# ============================================\n",
    "initialize_ee()\n",
    "ensure_out_dir(OUT_DIR)\n",
    "\n",
    "# Load Input Data (The consolidated file from 02a)\n",
    "print(f\"Loading full dataset: {INPUT_PKL}\")\n",
    "if not os.path.exists(INPUT_PKL):\n",
    "    raise FileNotFoundError(f\"Input file not found: {INPUT_PKL}. Please run 02a first.\")\n",
    "\n",
    "df = pd.read_pickle(INPUT_PKL)\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df['start_time'] = pd.to_datetime(df['start_time'], unit='s')\n",
    "df['end_time'] = pd.to_datetime(df['end_time'], unit='s')\n",
    "\n",
    "# Filter for IMERG Era (Post June 2000)\n",
    "# Events before this date will not have IMERG data\n",
    "df_valid = df[df['start_time'] >= IMERG_START_DATE].copy()\n",
    "\n",
    "print(f\"Total events in input: {len(df)}\")\n",
    "print(f\"Events valid for IMERG (post-2000): {len(df_valid)}\")\n",
    "\n",
    "# ============================================\n",
    "# 4) SMART BATCH PROCESSING (ID-BASED)\n",
    "# ============================================\n",
    "\n",
    "print(f\"--- PREPARING WORK PLAN ---\")\n",
    "\n",
    "# 1. Identify what is already done\n",
    "# We scan the output directory for existing RAIN batches to avoid re-processing\n",
    "processed_ids = set()\n",
    "pkl_pattern = os.path.join(OUT_DIR, \"imerg_batch_*.pkl\")\n",
    "existing_files = glob.glob(pkl_pattern)\n",
    "\n",
    "if existing_files:\n",
    "    print(f\"Found {len(existing_files)} existing batch files. Scanning for processed IDs...\")\n",
    "    for f in tqdm(existing_files, desc=\"Indexing existing data\"):\n",
    "        try:\n",
    "            # Only read columns needed for ID check to save memory\n",
    "            df_temp = pd.read_pickle(f)\n",
    "            if 'event_id' in df_temp.columns:\n",
    "                processed_ids.update(df_temp['event_id'].tolist())\n",
    "            del df_temp\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping corrupted file {f}: {e}\")\n",
    "\n",
    "print(f\"Total events already processed: {len(processed_ids)}\")\n",
    "\n",
    "# 2. Filter the Main DataFrame\n",
    "# We keep only rows whose ID is NOT in the processed set\n",
    "df_todo = df_valid[~df_valid['event_id'].isin(processed_ids)].copy()\n",
    "\n",
    "print(f\"Events remaining to process: {len(df_todo)}\")\n",
    "\n",
    "if len(df_todo) == 0:\n",
    "    print(\"All events are already processed! Nothing to do.\")\n",
    "else:\n",
    "    # 3. Process the remaining rows in new batches\n",
    "    start_batch_num = get_next_batch_index(OUT_DIR)\n",
    "    n_remaining = len(df_todo)\n",
    "    \n",
    "    # Calculate stop limit based on N_BATCHES_TO_RUN\n",
    "    max_rows_limit = N_BATCHES_TO_RUN * BATCH_SIZE\n",
    "    stop_at_row = min(n_remaining, max_rows_limit)\n",
    "\n",
    "    print(f\"Plan: Processing {min(N_BATCHES_TO_RUN, n_remaining // BATCH_SIZE + 1)} batches.\")\n",
    "    \n",
    "    # Iterate in chunks up to stop_at_row\n",
    "    for batch_i in range(0, stop_at_row, BATCH_SIZE):\n",
    "        \n",
    "        # Determine actual batch number for filename\n",
    "        current_file_num = start_batch_num + (batch_i // BATCH_SIZE)\n",
    "        \n",
    "        # Slice the TODO dataframe\n",
    "        batch_df = df_todo.iloc[batch_i : batch_i + BATCH_SIZE].copy()\n",
    "        \n",
    "        print(f\"\\nProcessing Batch {current_file_num} ({len(batch_df)} events)...\")\n",
    "        \n",
    "        matrices = []\n",
    "        masks = []\n",
    "        metas = []\n",
    "        \n",
    "        # Inner loop: iterate rows in current batch\n",
    "        for idx, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Batch {current_file_num}\"):\n",
    "            mat, msk, mt = extract_rain_data(row)\n",
    "            matrices.append(mat)\n",
    "            masks.append(msk)\n",
    "            metas.append(mt)\n",
    "        \n",
    "        # Assign results to columns\n",
    "        batch_df['imerg_matrix'] = matrices\n",
    "        batch_df['imerg_mask'] = masks\n",
    "        batch_df['imerg_meta'] = metas\n",
    "        \n",
    "        # Save batch to disk (Pickle)\n",
    "        out_path = os.path.join(OUT_DIR, f\"imerg_batch_{current_file_num:04d}.pkl\")\n",
    "        batch_df.to_pickle(out_path)\n",
    "        print(f\"Saved: {out_path}\")\n",
    "        \n",
    "        # === MEMORY CLEANUP ===\n",
    "        # Explicitly delete large objects to free RAM for next iteration\n",
    "        del batch_df\n",
    "        del matrices\n",
    "        del masks\n",
    "        del metas\n",
    "        gc.collect() # Force garbage collection\n",
    "        # ======================\n",
    "\n",
    "    print(\"\\n--- BATCH LIMIT REACHED ---\")\n",
    "    print(f\"Stopped execution after {N_BATCHES_TO_RUN} batches as requested.\")\n",
    "\n",
    "# ============================================\n",
    "# 5) FINAL MERGE\n",
    "# ============================================\n",
    "print(\"\\n--- FINALIZING ---\")\n",
    "print(\"Merging all batch files...\")\n",
    "\n",
    "pkl_pattern = os.path.join(OUT_DIR, \"imerg_batch_*.pkl\")\n",
    "all_pkl_files = glob.glob(pkl_pattern)\n",
    "\n",
    "if not all_pkl_files:\n",
    "    print(\"No output files found.\")\n",
    "else:\n",
    "    # Concatenate all batches\n",
    "    df_list = []\n",
    "    for f in tqdm(all_pkl_files, desc=\"Loading Batches\"):\n",
    "        try:\n",
    "            df_list.append(pd.read_pickle(f))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {f}: {e}\")\n",
    "            \n",
    "    if df_list:\n",
    "        df_results = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        print(\"Merging results back to main dataset...\")\n",
    "        \n",
    "        # Reload the original input again to ensure we have the clean base\n",
    "        df_base = pd.read_pickle(INPUT_PKL)\n",
    "        \n",
    "        # Merge the new IMERG columns onto the base dataframe\n",
    "        # Left join ensures we keep all original events\n",
    "        df_final = df_base.merge(\n",
    "            df_results[['event_id', 'imerg_matrix', 'imerg_mask', 'imerg_meta']], \n",
    "            on='event_id', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Save Final PKL\n",
    "        df_final.to_pickle(OUT_FINAL_PKL)\n",
    "        \n",
    "        print(f\"SUCCESS! Final dataset saved to: {OUT_FINAL_PKL}\")\n",
    "        \n",
    "        # Verification\n",
    "        if 'imerg_matrix' in df_final.columns:\n",
    "            count = df_final['imerg_matrix'].notnull().sum()\n",
    "            print(f\"Events with valid Rain Data: {count}\")\n",
    "    else:\n",
    "        print(\"Failed to load any batch files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faec413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e8993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2754f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINALIZING ---\n",
      "Merging all batch files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Batches: 100%|█████████████████████████████████████████████████████████████████| 99/99 [00:03<00:00, 25.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 58500 processed rain events (raw count).\n",
      "Merging results back to main dataset structure...\n",
      "\n",
      "[WARNING] Found 194 events with NULL rain data. Removing them...\n",
      "--- Dropped Events Log ---\n",
      "Removing ID: 1134 | Date: 01-11-2000 00:00\n",
      "Removing ID: 1546 | Date: 06-12-2000 00:00\n",
      "Removing ID: 1639 | Date: 13-12-2000 00:00\n",
      "Removing ID: 1668 | Date: 01-01-2001 00:00\n",
      "Removing ID: 1724 | Date: 01-01-2001 00:00\n",
      "Removing ID: 1788 | Date: 01-01-2001 00:00\n",
      "Removing ID: 1984 | Date: 01-05-2001 00:00\n",
      "Removing ID: 2697 | Date: 01-01-2002 00:00\n",
      "Removing ID: 4777 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4778 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4779 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4780 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4783 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4784 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4785 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4786 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4788 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4789 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4790 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4791 | Date: 02-09-2003 00:00\n",
      "Removing ID: 4792 | Date: 05-09-2003 00:00\n",
      "Removing ID: 4793 | Date: 05-09-2003 00:00\n",
      "Removing ID: 4794 | Date: 05-09-2003 00:00\n",
      "Removing ID: 4795 | Date: 05-09-2003 00:00\n",
      "Removing ID: 5324 | Date: 01-01-2004 00:00\n",
      "Removing ID: 5372 | Date: 01-01-2004 00:00\n",
      "Removing ID: 5556 | Date: 01-04-2004 00:00\n",
      "Removing ID: 5568 | Date: 01-04-2004 00:00\n",
      "Removing ID: 7275 | Date: 26-12-2004 00:00\n",
      "Removing ID: 7284 | Date: 26-12-2004 00:00\n",
      "Removing ID: 7415 | Date: 01-01-2005 00:00\n",
      "Removing ID: 7467 | Date: 01-01-2005 00:00\n",
      "Removing ID: 7546 | Date: 01-01-2005 00:00\n",
      "Removing ID: 7605 | Date: 08-01-2005 00:00\n",
      "Removing ID: 7609 | Date: 08-01-2005 00:00\n",
      "Removing ID: 9589 | Date: 01-01-2006 00:00\n",
      "Removing ID: 9637 | Date: 01-01-2006 00:00\n",
      "Removing ID: 9816 | Date: 18-02-2006 00:00\n",
      "Removing ID: 12345 | Date: 01-01-2007 00:00\n",
      "Removing ID: 12393 | Date: 01-01-2007 00:00\n",
      "Removing ID: 12466 | Date: 01-01-2007 00:00\n",
      "Removing ID: 12522 | Date: 09-01-2007 00:00\n",
      "Removing ID: 12523 | Date: 10-01-2007 00:00\n",
      "Removing ID: 12555 | Date: 16-01-2007 00:00\n",
      "Removing ID: 12558 | Date: 18-01-2007 00:00\n",
      "Removing ID: 12941 | Date: 01-05-2007 00:00\n",
      "Removing ID: 13599 | Date: 05-07-2007 00:00\n",
      "Removing ID: 15982 | Date: 01-01-2008 00:00\n",
      "Removing ID: 15996 | Date: 01-01-2008 00:00\n",
      "Removing ID: 16252 | Date: 15-01-2008 00:00\n",
      "Removing ID: 16256 | Date: 15-01-2008 00:00\n",
      "Removing ID: 16314 | Date: 03-02-2008 00:00\n",
      "Removing ID: 16377 | Date: 25-02-2008 00:00\n",
      "Removing ID: 19802 | Date: 16-11-2008 00:00\n",
      "Removing ID: 19948 | Date: 27-11-2008 00:00\n",
      "Removing ID: 20307 | Date: 01-01-2009 00:00\n",
      "Removing ID: 20476 | Date: 11-01-2009 00:00\n",
      "Removing ID: 21279 | Date: 01-05-2009 00:00\n",
      "Removing ID: 27695 | Date: 22-02-2010 00:00\n",
      "Removing ID: 27698 | Date: 22-02-2010 00:00\n",
      "Removing ID: 28228 | Date: 07-03-2010 00:00\n",
      "Removing ID: 28231 | Date: 07-03-2010 00:00\n",
      "Removing ID: 28250 | Date: 09-03-2010 00:00\n",
      "Removing ID: 28255 | Date: 09-03-2010 00:00\n",
      "Removing ID: 28261 | Date: 09-03-2010 00:00\n",
      "Removing ID: 28262 | Date: 09-03-2010 00:00\n",
      "Removing ID: 28266 | Date: 09-03-2010 00:00\n",
      "Removing ID: 28273 | Date: 11-03-2010 00:00\n",
      "Removing ID: 28274 | Date: 11-03-2010 00:00\n",
      "Removing ID: 28278 | Date: 11-03-2010 00:00\n",
      "Removing ID: 28279 | Date: 11-03-2010 00:00\n",
      "Removing ID: 28281 | Date: 11-03-2010 00:00\n",
      "Removing ID: 28492 | Date: 28-03-2010 00:00\n",
      "Removing ID: 28677 | Date: 04-04-2010 00:00\n",
      "Removing ID: 28772 | Date: 12-04-2010 00:00\n",
      "Removing ID: 28775 | Date: 12-04-2010 00:00\n",
      "Removing ID: 28787 | Date: 14-04-2010 00:00\n",
      "Removing ID: 28798 | Date: 14-04-2010 00:00\n",
      "Removing ID: 28802 | Date: 14-04-2010 00:00\n",
      "Removing ID: 28957 | Date: 27-04-2010 00:00\n",
      "Removing ID: 28964 | Date: 27-04-2010 00:00\n",
      "Removing ID: 28974 | Date: 28-04-2010 00:00\n",
      "Removing ID: 29524 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29531 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29533 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29560 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29571 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29585 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29610 | Date: 15-05-2010 00:00\n",
      "Removing ID: 29751 | Date: 16-05-2010 00:00\n",
      "Removing ID: 29821 | Date: 16-05-2010 00:00\n",
      "Removing ID: 29876 | Date: 17-05-2010 00:00\n",
      "Removing ID: 29913 | Date: 17-05-2010 00:00\n",
      "Removing ID: 29995 | Date: 18-05-2010 00:00\n",
      "Removing ID: 30115 | Date: 20-05-2010 00:00\n",
      "Removing ID: 30123 | Date: 20-05-2010 00:00\n",
      "Removing ID: 30129 | Date: 20-05-2010 00:00\n",
      "Removing ID: 30139 | Date: 20-05-2010 00:00\n",
      "Removing ID: 32496 | Date: 18-06-2010 00:00\n",
      "Removing ID: 36769 | Date: 15-09-2010 00:00\n",
      "Removing ID: 36780 | Date: 15-09-2010 00:00\n",
      "Removing ID: 38351 | Date: 16-10-2010 00:00\n",
      "Removing ID: 41551 | Date: 01-01-2011 00:00\n",
      "Removing ID: 41738 | Date: 01-01-2011 00:00\n",
      "Removing ID: 41762 | Date: 01-01-2011 00:00\n",
      "Removing ID: 41850 | Date: 01-01-2011 00:00\n",
      "Removing ID: 41877 | Date: 01-01-2011 00:00\n",
      "Removing ID: 41918 | Date: 01-01-2011 00:00\n",
      "Removing ID: 41936 | Date: 01-01-2011 00:00\n",
      "Removing ID: 43769 | Date: 26-02-2011 00:00\n",
      "Removing ID: 44491 | Date: 21-03-2011 00:00\n",
      "Removing ID: 44497 | Date: 22-03-2011 00:00\n",
      "Removing ID: 44498 | Date: 22-03-2011 00:00\n",
      "Removing ID: 44500 | Date: 22-03-2011 00:00\n",
      "Removing ID: 44541 | Date: 23-03-2011 00:00\n",
      "Removing ID: 44550 | Date: 23-03-2011 00:00\n",
      "Removing ID: 44560 | Date: 23-03-2011 00:00\n",
      "Removing ID: 44577 | Date: 23-03-2011 00:00\n",
      "Removing ID: 44585 | Date: 23-03-2011 00:00\n",
      "Removing ID: 44587 | Date: 23-03-2011 00:00\n",
      "Removing ID: 44887 | Date: 12-04-2011 00:00\n",
      "Removing ID: 44894 | Date: 12-04-2011 00:00\n",
      "Removing ID: 44902 | Date: 12-04-2011 00:00\n",
      "Removing ID: 46264 | Date: 09-06-2011 00:00\n",
      "Removing ID: 46292 | Date: 10-06-2011 00:00\n",
      "Removing ID: 46294 | Date: 10-06-2011 00:00\n",
      "Removing ID: 47917 | Date: 19-07-2011 00:00\n",
      "Removing ID: 47971 | Date: 20-07-2011 00:00\n",
      "Removing ID: 52728 | Date: 08-11-2011 00:00\n",
      "Removing ID: 52870 | Date: 15-11-2011 00:00\n",
      "Removing ID: 53345 | Date: 25-11-2011 00:00\n",
      "Removing ID: 53351 | Date: 25-11-2011 00:00\n",
      "Removing ID: 53356 | Date: 25-11-2011 00:00\n",
      "Removing ID: 53365 | Date: 25-11-2011 00:00\n",
      "Removing ID: 53443 | Date: 28-11-2011 00:00\n",
      "Removing ID: 53995 | Date: 26-12-2011 00:00\n",
      "Removing ID: 54091 | Date: 28-12-2011 00:00\n",
      "Removing ID: 54164 | Date: 01-01-2012 00:00\n",
      "Removing ID: 54206 | Date: 01-01-2012 00:00\n",
      "Removing ID: 55684 | Date: 22-02-2012 00:00\n",
      "Removing ID: 56047 | Date: 11-03-2012 00:00\n",
      "Removing ID: 56134 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56139 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56141 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56146 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56151 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56152 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56153 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56154 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56155 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56160 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56161 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56162 | Date: 22-03-2012 00:00\n",
      "Removing ID: 56221 | Date: 28-03-2012 00:00\n",
      "Removing ID: 56285 | Date: 01-04-2012 00:00\n",
      "Removing ID: 56302 | Date: 02-04-2012 00:00\n",
      "Removing ID: 56306 | Date: 02-04-2012 00:00\n",
      "Removing ID: 56314 | Date: 02-04-2012 00:00\n",
      "Removing ID: 56319 | Date: 02-04-2012 00:00\n",
      "Removing ID: 56332 | Date: 02-04-2012 00:00\n",
      "Removing ID: 56432 | Date: 09-04-2012 00:00\n",
      "Removing ID: 56434 | Date: 09-04-2012 00:00\n",
      "Removing ID: 56446 | Date: 09-04-2012 00:00\n",
      "Removing ID: 56450 | Date: 09-04-2012 00:00\n",
      "Removing ID: 56454 | Date: 09-04-2012 00:00\n",
      "Removing ID: 56537 | Date: 16-04-2012 00:00\n",
      "Removing ID: 56539 | Date: 16-04-2012 00:00\n",
      "Removing ID: 56560 | Date: 16-04-2012 00:00\n",
      "Removing ID: 56562 | Date: 16-04-2012 00:00\n",
      "Removing ID: 56564 | Date: 16-04-2012 00:00\n",
      "Removing ID: 56593 | Date: 18-04-2012 00:00\n",
      "Removing ID: 56594 | Date: 18-04-2012 00:00\n",
      "Removing ID: 56600 | Date: 18-04-2012 00:00\n",
      "Removing ID: 56607 | Date: 18-04-2012 00:00\n",
      "Removing ID: 56623 | Date: 18-04-2012 00:00\n",
      "Removing ID: 56647 | Date: 21-04-2012 00:00\n",
      "Removing ID: 56662 | Date: 21-04-2012 00:00\n",
      "Removing ID: 56679 | Date: 21-04-2012 00:00\n",
      "Removing ID: 56681 | Date: 21-04-2012 00:00\n",
      "Removing ID: 56686 | Date: 21-04-2012 00:00\n",
      "Removing ID: 56712 | Date: 24-04-2012 00:00\n",
      "Removing ID: 56717 | Date: 24-04-2012 00:00\n",
      "Removing ID: 56737 | Date: 24-04-2012 00:00\n",
      "Removing ID: 56743 | Date: 24-04-2012 00:00\n",
      "Removing ID: 56778 | Date: 27-04-2012 00:00\n",
      "Removing ID: 56779 | Date: 27-04-2012 00:00\n",
      "Removing ID: 56794 | Date: 27-04-2012 00:00\n",
      "Removing ID: 56813 | Date: 27-04-2012 00:00\n",
      "Removing ID: 56819 | Date: 27-04-2012 00:00\n",
      "Removing ID: 56881 | Date: 01-05-2012 00:00\n",
      "Removing ID: 57168 | Date: 20-05-2012 00:00\n",
      "Removing ID: 57348 | Date: 23-05-2012 00:00\n",
      "Removing ID: 57502 | Date: 28-05-2012 00:00\n",
      "Removing ID: 57720 | Date: 05-06-2012 00:00\n",
      "--- Cleaned. Remaining events: 58306 ---\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5) FINAL MERGE & CLEANUP\n",
    "# ============================================\n",
    "print(\"\\n--- FINALIZING ---\")\n",
    "print(\"Merging all batch files...\")\n",
    "\n",
    "pkl_pattern = os.path.join(OUT_DIR, \"imerg_batch_*.pkl\")\n",
    "all_pkl_files = glob.glob(pkl_pattern)\n",
    "\n",
    "if not all_pkl_files:\n",
    "    print(\"No output files found.\")\n",
    "else:\n",
    "    # Concatenate all batches\n",
    "    df_list = []\n",
    "    for f in tqdm(all_pkl_files, desc=\"Loading Batches\"):\n",
    "        try:\n",
    "            df_list.append(pd.read_pickle(f))\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {f}: {e}\")\n",
    "            \n",
    "    if df_list:\n",
    "        # This DataFrame contains ONLY the processed events with rain data\n",
    "        df_results = pd.concat(df_list, ignore_index=True)\n",
    "        \n",
    "        print(f\"Loaded {len(df_results)} processed rain events (raw count).\")\n",
    "        print(\"Merging results back to main dataset structure...\")\n",
    "        \n",
    "        # Reload the original input again to ensure we have the clean base columns\n",
    "        df_base = pd.read_pickle(INPUT_PKL)\n",
    "        \n",
    "        # Merge: 'inner' keeps only keys that appear in BOTH DataFrames\n",
    "        df_final = df_base.merge(\n",
    "            df_results[['event_id', 'imerg_matrix', 'imerg_mask', 'imerg_meta']], \n",
    "            on='event_id', \n",
    "            how='inner' \n",
    "        )\n",
    "        \n",
    "        # --- NEW: FILTER OUT NULL RAIN DATA ---\n",
    "        \n",
    "        # 1. Ensure start_time is datetime format for readable printing\n",
    "        if 'start_time' in df_final.columns:\n",
    "            df_final['start_time'] = pd.to_datetime(df_final['start_time'], unit='s')\n",
    "\n",
    "        # 2. Identify rows where 'imerg_matrix' is Null/None (Values of 0 are NOT null, so they stay)\n",
    "        missing_rain_mask = df_final['imerg_matrix'].isnull()\n",
    "        missing_events = df_final[missing_rain_mask]\n",
    "        \n",
    "        # 3. Print dates of dropped events\n",
    "        if not missing_events.empty:\n",
    "            print(f\"\\n[WARNING] Found {len(missing_events)} events with NULL rain data. Removing them...\")\n",
    "            print(\"--- Dropped Events Log ---\")\n",
    "            for idx, row in missing_events.iterrows():\n",
    "                try:\n",
    "                    d_str = row['start_time'].strftime('%d-%m-%Y %H:%M')\n",
    "                except:\n",
    "                    d_str = str(row['start_time'])\n",
    "                print(f\"Removing ID: {row['event_id']} | Date: {d_str}\")\n",
    "            \n",
    "            # 4. Perform the drop\n",
    "            df_final = df_final[~missing_rain_mask].copy()\n",
    "            print(f\"--- Cleaned. Remaining events: {len(df_final)} ---\")\n",
    "        else:\n",
    "            print(\"No NULL rain events found. All processed events are valid.\")\n",
    "\n",
    "        # --------------------------------------\n",
    "\n",
    "        # Save Final PKL (Cleaned subset)\n",
    "        df_final.to_pickle(OUT_FINAL_PKL)\n",
    "        \n",
    "        print(f\"SUCCESS! Final dataset saved to: {OUT_FINAL_PKL}\")\n",
    "        \n",
    "        # Final Verification\n",
    "        if 'imerg_matrix' in df_final.columns:\n",
    "            count = df_final['imerg_matrix'].notnull().sum()\n",
    "            print(f\"Verified valid Rain Data events: {count}\")\n",
    "    else:\n",
    "        print(\"Failed to load any batch files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08c7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4791d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
