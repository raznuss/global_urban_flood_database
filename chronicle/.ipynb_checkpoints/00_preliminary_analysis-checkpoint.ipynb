{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4712c29c",
   "metadata": {},
   "source": [
    "# Chronicle Flood Database Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Chronicle urban flood dataset, containing over 880,000 flood events worldwide from 2000-2025.\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: Chronicle preliminary dataset (HUJI)\n",
    "- **Records**: 882,972 flood events\n",
    "- **Time Period**: 2000-2025\n",
    "- **Geographic Coverage**: Global\n",
    "- **Data Format**: CSV with WKT geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e22ace",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba6c1382",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31192\\679377353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Core data analysis libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# numpy compat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_numpy_dev\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_is_numpy_dev\u001b[0m  \u001b[1;31m# pyright: ignore # noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_typing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m from pandas.compat.numpy import (\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mis_numpy_dev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnp_version_under1p21\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# numpy versioning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# pyright: reportUnusedImport = false\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m from pandas.util._decorators import (  # noqa:F401\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mAppender\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mSubstitution\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcache_readonly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproperties\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcache_readonly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m from pandas._typing import (\n\u001b[0;32m     16\u001b[0m     \u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterval\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m from pandas._libs.tslibs import (\n\u001b[0;32m     15\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\interval.pyx\u001b[0m in \u001b[0;36minit pandas._libs.interval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "# Core data analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Geospatial libraries\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from shapely import wkt\n",
    "from matplotlib.patches import Polygon as MplPolygon, Rectangle, Circle\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923b6f2",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3814ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Chronicle dataset\n",
    "file_path = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\chronicle_preliminary_huji.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded successfully: {df.shape[0]:,} records, {df.shape[1]} columns\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b18e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset structure overview\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe49aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample records\n",
    "print(\"Sample records:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4742df5d",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefcb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data completeness and quality checks\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(f\"\\nMissing values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data[missing_data > 0])\n",
    "\n",
    "print(f\"\\nDuplicate records: {df.duplicated().sum():,}\")\n",
    "print(f\"Unique UUIDs: {df['uuid'].nunique():,} (should match total records)\")\n",
    "\n",
    "print(f\"\\nVersion distribution:\")\n",
    "print(df['version'].value_counts())\n",
    "\n",
    "print(f\"\\nGeometry types:\")\n",
    "geometry_types = df['geometry_wkt'].str.extract(r'^(\\w+)')[0].value_counts()\n",
    "print(geometry_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca3006",
   "metadata": {},
   "source": [
    "## 4. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps and analyze temporal patterns\n",
    "print(\"Temporal Analysis:\")\n",
    "\n",
    "# Time range analysis\n",
    "min_start = datetime.fromtimestamp(df['start_time'].min())\n",
    "max_start = datetime.fromtimestamp(df['start_time'].max())\n",
    "min_end = datetime.fromtimestamp(df['end_time'].min())\n",
    "max_end = datetime.fromtimestamp(df['end_time'].max())\n",
    "\n",
    "print(f\"\\nTime coverage:\")\n",
    "print(f\"Start times: {min_start.strftime('%Y-%m-%d')} to {max_start.strftime('%Y-%m-%d')}\")\n",
    "print(f\"End times: {min_end.strftime('%Y-%m-%d')} to {max_end.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Duration analysis\n",
    "duration_stats = df['duration_days'].describe()\n",
    "print(f\"\\nDuration statistics (days):\")\n",
    "print(f\"Range: {df['duration_days'].min()} to {df['duration_days'].max()}\")\n",
    "print(f\"Mean: {duration_stats['mean']:.2f}, Median: {duration_stats['50%']:.2f}\")\n",
    "\n",
    "print(f\"\\nMost common durations:\")\n",
    "print(df['duration_days'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41605d",
   "metadata": {},
   "source": [
    "## 5. Spatial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c6e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Set the threshold area in square kilometers\n",
    "# You can try changing this to 0.1, 0.5, or keep it at 1.0\n",
    "THRESHOLD_KM2 = 0.25\n",
    "# ---------------------\n",
    "\n",
    "print(\"--- Flood Area Analysis & Threshold Check ---\")\n",
    "\n",
    "# 1. General Statistics\n",
    "area_stats = df['area_km2'].describe()\n",
    "print(f\"\\n1. General Statistics (km²):\")\n",
    "print(f\"   Range: {df['area_km2'].min():.6f} to {df['area_km2'].max():.2f}\")\n",
    "print(f\"   Mean: {area_stats['mean']:.2f}\")\n",
    "print(f\"   Median: {area_stats['50%']:.2f}\")\n",
    "\n",
    "# 2. Threshold Analysis\n",
    "# Split the data based on the threshold\n",
    "small_polygons = df[df['area_km2'] < THRESHOLD_KM2]\n",
    "valid_polygons = df[df['area_km2'] >= THRESHOLD_KM2]\n",
    "\n",
    "count_total = len(df)\n",
    "count_small = len(small_polygons)\n",
    "count_valid = len(valid_polygons)\n",
    "\n",
    "print(f\"\\n2. Threshold Analysis (Cutoff: {THRESHOLD_KM2} km²):\")\n",
    "print(f\"   Total Events: {count_total}\")\n",
    "print(f\"   [+] Valid Events (>= {THRESHOLD_KM2} km²): {count_valid} ({count_valid/count_total:.1%})\")\n",
    "print(f\"   [-] Too Small Events (< {THRESHOLD_KM2} km²): {count_small} ({count_small/count_total:.1%})\")\n",
    "\n",
    "# 3. Percentiles (To see where the threshold lands)\n",
    "percentiles = [10, 25, 50, 75, 90]\n",
    "print(f\"\\n3. Area Percentiles (km²):\")\n",
    "for p in percentiles:\n",
    "    value = np.percentile(df['area_km2'], p)\n",
    "    marker = \" <--- THRESHOLD IS HERE\" if (value >= THRESHOLD_KM2 and np.percentile(df['area_km2'], p-10 if p>10 else 0) < THRESHOLD_KM2) else \"\"\n",
    "    print(f\"   {p}th percentile: {value:.4f} {marker}\")\n",
    "\n",
    "# 4. Recommendation based on 100m resolution\n",
    "# A 100m pixel covers 0.01 km². Ideally, we want at least 4-9 pixels for minimal validity.\n",
    "min_recommended_area = 0.04 # 4 pixels\n",
    "print(f\"\\n--- Technical Note ---\")\n",
    "print(f\"With 100m resolution, 1 pixel = 0.01 km².\")\n",
    "print(f\"Your chosen threshold of {THRESHOLD_KM2} km² gives ~{int(THRESHOLD_KM2/0.01)} pixels per polygon.\")\n",
    "if THRESHOLD_KM2 >= 0.1:\n",
    "    print(\"Verdict: Excellent accuracy expected.\")\n",
    "else:\n",
    "    print(\"Verdict: Borderline accuracy (few pixels per polygon).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416365d9",
   "metadata": {},
   "source": [
    "## 6. Statistical Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97982c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive statistical visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Chronicle Urban Flood Dataset - Statistical Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Flood area distribution (log scale)\n",
    "axes[0,0].hist(df['area_km2'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0,0].set_xlabel('Flood Area (km²)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Distribution of Flood Areas')\n",
    "axes[0,0].set_yscale('log')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Duration distribution\n",
    "duration_counts = df['duration_days'].value_counts().sort_index()\n",
    "axes[0,1].bar(duration_counts.index, duration_counts.values, color='coral', alpha=0.8)\n",
    "axes[0,1].set_xlabel('Duration (days)')\n",
    "axes[0,1].set_ylabel('Number of Events')\n",
    "axes[0,1].set_title('Flood Duration Distribution')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Temporal distribution by year\n",
    "df_temp = df.copy()\n",
    "df_temp['year'] = pd.to_datetime(df_temp['start_time'], unit='s').dt.year\n",
    "yearly_counts = df_temp['year'].value_counts().sort_index()\n",
    "\n",
    "axes[1,0].plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2, markersize=4)\n",
    "axes[1,0].set_xlabel('Year')\n",
    "axes[1,0].set_ylabel('Number of Flood Events')\n",
    "axes[1,0].set_title('Flood Events Over Time')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Area vs Duration relationship\n",
    "sample_df = df.sample(n=5000, random_state=42)\n",
    "scatter = axes[1,1].scatter(sample_df['duration_days'], sample_df['area_km2'], \n",
    "                           alpha=0.6, s=20, c='purple')\n",
    "axes[1,1].set_xlabel('Duration (days)')\n",
    "axes[1,1].set_ylabel('Flood Area (km²)')\n",
    "axes[1,1].set_title('Area vs Duration (5K sample)')\n",
    "axes[1,1].set_yscale('log')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"Temporal coverage: {yearly_counts.index.min()} - {yearly_counts.index.max()}\")\n",
    "print(f\"Peak year: {yearly_counts.idxmax()} ({yearly_counts.max():,} events)\")\n",
    "print(f\"Most common duration: {duration_counts.idxmax()} day(s) ({duration_counts.max():,} events)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5987ed5",
   "metadata": {},
   "source": [
    "## 7. Geographic Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83caed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from geometry for geographic analysis\n",
    "def extract_centroid_coordinates(geometry_wkt_string):\n",
    "    \"\"\"Extract centroid coordinates from WKT geometry string.\"\"\"\n",
    "    try:\n",
    "        if geometry_wkt_string.startswith(('POLYGON', 'MULTIPOLYGON')):\n",
    "            geom = wkt.loads(geometry_wkt_string)\n",
    "            centroid = geom.centroid\n",
    "            return centroid.y, centroid.x  # lat, lon\n",
    "    except Exception:\n",
    "        return None, None\n",
    "    return None, None\n",
    "\n",
    "# Process sample for geographic visualization\n",
    "sample_size = 9000\n",
    "sample_events = df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "print(f\"Processing {sample_size} events for geographic analysis...\")\n",
    "\n",
    "coordinates = []\n",
    "areas = []\n",
    "durations = []\n",
    "\n",
    "for idx, row in sample_events.iterrows():\n",
    "    lat, lon = extract_centroid_coordinates(row['geometry_wkt'])\n",
    "    if lat is not None and lon is not None:\n",
    "        coordinates.append([lon, lat])  # lon, lat order\n",
    "        areas.append(row['area_km2'])\n",
    "        durations.append(row['duration_days'])\n",
    "\n",
    "coordinates = np.array(coordinates)\n",
    "lons = coordinates[:, 0]\n",
    "lats = coordinates[:, 1]\n",
    "areas_array = np.array(areas)\n",
    "\n",
    "print(f\"Successfully processed {len(coordinates)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c5ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create global distribution maps\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Map 1: Distribution by duration\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection=ccrs.PlateCarree())\n",
    "ax1.set_global()\n",
    "ax1.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax1.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "ax1.add_feature(cfeature.LAND, alpha=0.2, color='lightgray')\n",
    "ax1.add_feature(cfeature.OCEAN, alpha=0.2, color='lightblue')\n",
    "\n",
    "scatter1 = ax1.scatter(lons, lats, c=durations, s=30, alpha=0.7, \n",
    "                      cmap='viridis', transform=ccrs.PlateCarree(), \n",
    "                      edgecolors='black', linewidth=0.3)\n",
    "ax1.set_title('Global Flood Distribution by Duration', fontsize=14, fontweight='bold')\n",
    "ax1.gridlines(draw_labels=True, alpha=0.3)\n",
    "\n",
    "cbar1 = plt.colorbar(scatter1, ax=ax1, shrink=0.6, pad=0.05)\n",
    "cbar1.set_label('Duration (days)', fontsize=12)\n",
    "\n",
    "# Map 2: Distribution by area\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection=ccrs.PlateCarree())\n",
    "ax2.set_global()\n",
    "ax2.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax2.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "ax2.add_feature(cfeature.LAND, alpha=0.2, color='lightgray')\n",
    "ax2.add_feature(cfeature.OCEAN, alpha=0.2, color='lightblue')\n",
    "\n",
    "sizes = np.log10(areas_array + 1) * 15\n",
    "scatter2 = ax2.scatter(lons, lats, c=areas_array, s=sizes, alpha=0.7, \n",
    "                      cmap='Reds', transform=ccrs.PlateCarree(),\n",
    "                      edgecolors='black', linewidth=0.3)\n",
    "ax2.set_title('Global Flood Distribution by Area', fontsize=14, fontweight='bold')\n",
    "ax2.gridlines(draw_labels=True, alpha=0.3)\n",
    "\n",
    "cbar2 = plt.colorbar(scatter2, ax=ax2, shrink=0.6, pad=0.05)\n",
    "cbar2.set_label('Flood Area (km²)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic statistics\n",
    "print(\"Geographic Distribution Summary:\")\n",
    "print(f\"Latitude range: {lats.min():.2f}° to {lats.max():.2f}°\")\n",
    "print(f\"Longitude range: {lons.min():.2f}° to {lons.max():.2f}°\")\n",
    "\n",
    "# Hemisphere distribution\n",
    "northern_hem = np.sum(lats > 0)\n",
    "southern_hem = np.sum(lats <= 0)\n",
    "eastern_hem = np.sum(lons > 0)  \n",
    "western_hem = np.sum(lons <= 0)\n",
    "\n",
    "print(f\"\\nHemisphere Distribution:\")\n",
    "print(f\"Northern Hemisphere: {northern_hem:,} events ({northern_hem/len(lats)*100:.1f}%)\")\n",
    "print(f\"Southern Hemisphere: {southern_hem:,} events ({southern_hem/len(lats)*100:.1f}%)\")\n",
    "print(f\"Eastern Hemisphere: {eastern_hem:,} events ({eastern_hem/len(lons)*100:.1f}%)\")\n",
    "print(f\"Western Hemisphere: {western_hem:,} events ({western_hem/len(lons)*100:.1f}%)\")\n",
    "\n",
    "# Continental approximation\n",
    "continents = {\n",
    "    'Europe': np.sum((lats > 35) & (lats < 72) & (lons > -10) & (lons < 40)),\n",
    "    'Asia': np.sum((lats > 10) & (lats < 72) & (lons > 40) & (lons < 180)),\n",
    "    'North America': np.sum((lats > 15) & (lats < 72) & (lons > -170) & (lons < -50)),\n",
    "    'Africa': np.sum((lats > -35) & (lats < 35) & (lons > -20) & (lons < 55)),\n",
    "    'South America': np.sum((lats > -55) & (lats < 15) & (lons > -85) & (lons < -35)),\n",
    "    'Oceania': np.sum((lats > -50) & (lats < 0) & (lons > 110) & (lons < 180))\n",
    "}\n",
    "\n",
    "print(f\"\\nApproximate Continental Distribution:\")\n",
    "for continent, count in sorted(continents.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        print(f\"{continent}: {count:,} events ({count/len(lats)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87a075",
   "metadata": {},
   "source": [
    "## 8. Case Study: Detailed Event Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40827e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Event Selection - Choose any event from 1 to 882,972\n",
    "\n",
    "EVENT_NUMBER = 0  # *** CHANGE THIS NUMBER ***\n",
    "\n",
    "print(f\"Analyzing event number {EVENT_NUMBER:,} out of {len(df):,} total events\")\n",
    "print(\"To analyze a different event, change EVENT_NUMBER above and re-run the cells below\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get the selected event (convert from 1-based to 0-based index)\n",
    "sample_event = df.iloc[EVENT_NUMBER - 1]\n",
    "\n",
    "print(\"Case Study: Detailed Flood Event Analysis\")\n",
    "\n",
    "# Event details\n",
    "start_date = datetime.fromtimestamp(sample_event['start_time'])\n",
    "end_date = datetime.fromtimestamp(sample_event['end_time'])\n",
    "\n",
    "print(f\"Event ID: {sample_event['uuid']}\")\n",
    "print(f\"Area: {sample_event['area_km2']:.2f} km²\")\n",
    "print(f\"Duration: {sample_event['duration_days']} days\")\n",
    "print(f\"Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Version: {sample_event['version']}\")\n",
    "\n",
    "# Parse geometry\n",
    "geometry_wkt = sample_event['geometry_wkt']\n",
    "geom = wkt.loads(geometry_wkt)\n",
    "centroid = geom.centroid\n",
    "center_lat, center_lon = centroid.y, centroid.x\n",
    "bounds = geom.bounds\n",
    "\n",
    "print(f\"\\nGeographic Information:\")\n",
    "print(f\"Center: {center_lat:.4f}°N, {center_lon:.4f}°E\")\n",
    "print(f\"Bounding box: {bounds}\")\n",
    "print(f\"Geometry type: {geometry_wkt.split('(')[0]}\")\n",
    "\n",
    "# Statistical context\n",
    "area_rank = (df['area_km2'] >= sample_event['area_km2']).sum()\n",
    "area_percentile = (df['area_km2'] <= sample_event['area_km2']).sum() / len(df) * 100\n",
    "duration_percentile = (df['duration_days'] <= sample_event['duration_days']).sum() / len(df) * 100\n",
    "\n",
    "print(f\"\\nStatistical Context:\")\n",
    "print(f\"Area rank: {area_rank:,} out of {len(df):,} events\")\n",
    "print(f\"Area percentile: {area_percentile:.1f}%\")\n",
    "print(f\"Duration percentile: {duration_percentile:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657f507",
   "metadata": {},
   "source": [
    "## 9. Flood Event Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620d42e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "center_lat, center_lon = centroid.y, centroid.x\n",
    "area_km2 = sample_event[\"area_km2\"]\n",
    "\n",
    "# Padding around the flood polygon for the local map\n",
    "if area_km2 < 1:\n",
    "    padding_deg = 0.02\n",
    "elif area_km2 < 10:\n",
    "    padding_deg = 0.03\n",
    "elif area_km2 < 100:\n",
    "    padding_deg = 0.05\n",
    "else:\n",
    "    padding_deg = 0.08\n",
    "\n",
    "local_west  = bounds[0] - padding_deg\n",
    "local_east  = bounds[2] + padding_deg\n",
    "local_south = bounds[1] - padding_deg\n",
    "local_north = bounds[3] + padding_deg\n",
    "\n",
    "print(\n",
    "    f\"Local map bounds: {local_west:.4f} to {local_east:.4f} (lon), \"\n",
    "    f\"{local_south:.4f} to {local_north:.4f} (lat)\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 9.1 Global context map (static)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "ax.set_global()\n",
    "\n",
    "# Basic background\n",
    "ax.add_feature(cfeature.LAND, alpha=0.3, color=\"lightgray\")\n",
    "ax.add_feature(cfeature.OCEAN, alpha=0.3, color=\"lightblue\")\n",
    "ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
    "ax.add_feature(cfeature.BORDERS, linewidth=0.3)\n",
    "\n",
    "# Flood location\n",
    "ax.plot(\n",
    "    center_lon,\n",
    "    center_lat,\n",
    "    marker=\"o\",\n",
    "    markersize=10,\n",
    "    markerfacecolor=\"red\",\n",
    "    markeredgecolor=\"darkred\",\n",
    "    markeredgewidth=2,\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "\n",
    "ax.set_title(\n",
    "    f\"Global context of flood event #{EVENT_NUMBER}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "info_text = (\n",
    "    f\"Event #{EVENT_NUMBER}\\n\"\n",
    "    f\"Area: {area_km2:.1f} km²\\n\"\n",
    "    f\"{center_lat:.2f}°N, {center_lon:.2f}°E\"\n",
    ")\n",
    "ax.text(\n",
    "    0.02,\n",
    "    0.98,\n",
    "    info_text,\n",
    "    transform=ax.transAxes,\n",
    "    fontsize=11,\n",
    "    va=\"top\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.4\", facecolor=\"white\", alpha=0.8),\n",
    ")\n",
    "\n",
    "ax.gridlines(draw_labels=True, alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 9.2 Local detailed map with “Google style” background (folium)\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import folium\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "def estimate_zoom(area_km2_value):\n",
    "    \"\"\"Heuristic zoom level based on flood area.\"\"\"\n",
    "    if area_km2_value < 1:\n",
    "        return 15\n",
    "    elif area_km2_value < 10:\n",
    "        return 14\n",
    "    elif area_km2_value < 100:\n",
    "        return 13\n",
    "    elif area_km2_value < 1000:\n",
    "        return 12\n",
    "    else:\n",
    "        return 11\n",
    "\n",
    "zoom_start = estimate_zoom(area_km2)\n",
    "print(f\"Estimated zoom level for local map: {zoom_start}\")\n",
    "\n",
    "# Create local interactive map\n",
    "local_map = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=zoom_start,\n",
    "    tiles=None,\n",
    ")\n",
    "\n",
    "# Streets layer good for seeing urban vs non urban\n",
    "folium.TileLayer(\n",
    "    tiles=\"https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",\n",
    "    attr=\"© OpenStreetMap contributors\",\n",
    "    name=\"Streets\",\n",
    "    control=True,\n",
    ").add_to(local_map)\n",
    "\n",
    "# Satellite layer similar to Google Maps\n",
    "folium.TileLayer(\n",
    "    tiles=(\n",
    "        \"https://server.arcgisonline.com/ArcGIS/rest/services/\"\n",
    "        \"World_Imagery/MapServer/tile/{z}/{y}/{x}\"\n",
    "    ),\n",
    "    attr=(\n",
    "        \"Tiles © Esri. Sources: Esri, i-cubed, USDA, USGS, AEX, GeoEye, \"\n",
    "        \"Getmapping, Aerogrid, IGN, IGP, UPR-EGP, and the GIS User Community\"\n",
    "    ),\n",
    "    name=\"Satellite\",\n",
    "    overlay=False,\n",
    "    control=True,\n",
    ").add_to(local_map)\n",
    "\n",
    "# Fit map to the flood bounding box\n",
    "local_map.fit_bounds([[local_south, local_west], [local_north, local_east]])\n",
    "\n",
    "# Add flood polygon\n",
    "try:\n",
    "    folium.GeoJson(\n",
    "        data=mapping(geom),\n",
    "        name=\"Flood polygon\",\n",
    "        style_function=lambda feature: {\n",
    "            \"color\": \"red\",\n",
    "            \"weight\": 3,\n",
    "            \"fillColor\": \"red\",\n",
    "            \"fillOpacity\": 0.35,\n",
    "        },\n",
    "        highlight_function=lambda feature: {\n",
    "            \"weight\": 4,\n",
    "            \"color\": \"yellow\",\n",
    "            \"fillOpacity\": 0.5,\n",
    "        },\n",
    "        tooltip=f\"Flood area: {area_km2:.1f} km²\",\n",
    "    ).add_to(local_map)\n",
    "\n",
    "    print(\"Added flood polygon to local map\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not add polygon, falling back to point marker: {e}\")\n",
    "    folium.CircleMarker(\n",
    "        location=[center_lat, center_lon],\n",
    "        radius=10,\n",
    "        color=\"red\",\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=f\"Flood centroid. Area: {area_km2:.1f} km²\",\n",
    "    ).add_to(local_map)\n",
    "\n",
    "# Add centroid marker\n",
    "folium.Marker(\n",
    "    location=[center_lat, center_lon],\n",
    "    icon=folium.Icon(color=\"yellow\", icon=\"star\", prefix=\"fa\"),\n",
    "    tooltip=f\"Centroid: {center_lat:.3f}N, {center_lon:.3f}E\",\n",
    ").add_to(local_map)\n",
    "\n",
    "# Add dashed rectangle showing the local extent\n",
    "folium.Rectangle(\n",
    "    bounds=[[local_south, local_west], [local_north, local_east]],\n",
    "    color=\"black\",\n",
    "    weight=1,\n",
    "    fill=False,\n",
    "    dash_array=\"5,5\",\n",
    "    tooltip=\"Local map extent\",\n",
    ").add_to(local_map)\n",
    "\n",
    "# Layer control\n",
    "folium.LayerControl(collapsed=False).add_to(local_map)\n",
    "\n",
    "print(\"Local interactive map ready. Use Streets or Satellite to inspect urban vs non urban context.\")\n",
    "\n",
    "local_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba2ca88",
   "metadata": {},
   "source": [
    "## 10. Event Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112de22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected Event Analysis Summary\n",
    "print(f\"Selected Event #{EVENT_NUMBER} - Complete Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nEvent Details:\")\n",
    "print(f\"• UUID: {sample_event['uuid']}\")\n",
    "print(f\"• Date: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"• Duration: {sample_event['duration_days']} days\")\n",
    "print(f\"• Area: {sample_event['area_km2']:.2f} km²\")\n",
    "print(f\"• Location: {center_lat:.4f}°N, {center_lon:.4f}°E\")\n",
    "\n",
    "print(f\"\\nStatistical Context:\")\n",
    "print(f\"• Area rank: {area_rank:,} out of {len(df):,} total events\")\n",
    "print(f\"• Area percentile: {area_percentile:.1f}% (larger than {area_percentile:.1f}% of all events)\")\n",
    "print(f\"• Duration percentile: {duration_percentile:.1f}%\")\n",
    "\n",
    "print(f\"\\nGeographic Context:\")\n",
    "print(f\"• Bounding box: {bounds}\")\n",
    "print(f\"• Geometry type: {geometry_wkt.split('(')[0]}\")\n",
    "\n",
    "# Determine geographic region\n",
    "if -180 <= center_lon <= -30:\n",
    "    region = \"Americas\"\n",
    "elif -30 < center_lon <= 60:\n",
    "    if 35 <= center_lat <= 72:\n",
    "        region = \"Europe\"\n",
    "    elif -35 <= center_lat < 35:\n",
    "        region = \"Africa/Middle East\"\n",
    "    else:\n",
    "        region = \"Other\"\n",
    "elif 60 < center_lon <= 180:\n",
    "    if center_lat > 10:\n",
    "        region = \"Asia\"\n",
    "    else:\n",
    "        region = \"Oceania\"\n",
    "else:\n",
    "    region = \"Unknown\"\n",
    "\n",
    "print(f\"• Approximate region: {region}\")\n",
    "\n",
    "print(f\"\\nTo analyze a different event:\")\n",
    "print(f\"• Change EVENT_NUMBER (currently {EVENT_NUMBER}) in the selection cell above\")\n",
    "print(f\"• Choose any number from 1 to {len(df):,}\")\n",
    "print(f\"• Re-run the analysis cells to see the new event\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
