{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6ae7e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chronicle rainfall data: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\chronicle_urban_df_with_IMERG_FULL.pkl\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 03b_chronicle_rain_analysis_and_standardization\n",
    "# Actions: \n",
    "# 1. Standardize area (Mollweide) and recalculate PFDI for consistency.\n",
    "# 2. Perform Spatial-Temporal Rainfall Intensity Analysis (30m - 24h).\n",
    "# Note: Includes a 'Clean Slate' step to remove polluted intensity columns.\n",
    "# =================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "RAIN_INPUT_PATH = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\chronicle_urban_df_with_IMERG_FULL.pkl\"\n",
    "RAIN_MASTER_FILE_PATH = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\chronicle_rain_master.pkl\"\n",
    "\n",
    "# Durations in minutes for peak intensity calculation\n",
    "DURATIONS = [30, 60, 120, 240, 360, 720, 1440]\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "print(f\"Loading chronicle rainfall data: {RAIN_INPUT_PATH}\")\n",
    "chronicle_events = pd.read_pickle(RAIN_INPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716335fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'uuid', 'area_km2', 'version', 'start_time', 'end_time',\n",
       "       'duration_days', 'geometry_wkt', 'urban_built_up_area_m2',\n",
       "       'polygon_total_area_m2', 'urban_percentage', 'event_id',\n",
       "       'poly_area_km2', 'upa_max', 'upa_p95', 'upa_p99', 'PFDI_p95',\n",
       "       'PFDI_p99', 'PFDI_max', 'imerg_matrix', 'imerg_mask', 'imerg_meta'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chronicle_events.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce4c6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Standardizing area and syncing PFDI metrics (p95, p99, max)...\n",
      "Step 2: Calculating Peak Intensities for 633339 events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 633339/633339 [05:49<00:00, 1811.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Complete. Total events: 633339\n",
      "SUCCESS! Clean master dataset saved to: D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\imerg_rain_outputs\\chronicle_rain_master.pkl\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# STEP 0: CLEAN SLATE (PREVENT COLUMN POLLUTION)\n",
    "# =================================================================\n",
    "# Removing any old intensity columns to ensure we don't carry over logic errors\n",
    "old_intensity_cols = [c for c in chronicle_events.columns if 'max_rainfall_intens' in c]\n",
    "if old_intensity_cols:\n",
    "    print(f\"Removing {len(old_intensity_cols)} existing intensity columns for a clean run...\")\n",
    "    chronicle_events = chronicle_events.drop(columns=old_intensity_cols)\n",
    "\n",
    "# =================================================================\n",
    "# ACTION 1: AREA STANDARDIZATION & MULTI-PFDI SYNC\n",
    "# =================================================================\n",
    "print(\"Step 1: Standardizing area and syncing PFDI metrics (p95, p99, max)...\")\n",
    "\n",
    "# Update master area to km2 using the precision Mollweide area\n",
    "chronicle_events['area_km2'] = chronicle_events['polygon_total_area_m2'] / 1e6\n",
    "\n",
    "# Synchronizing all PFDI metrics with the new standardized area\n",
    "metrics_to_sync = [\n",
    "    ('upa_p95', 'PFDI_p95'),\n",
    "    ('upa_p99', 'PFDI_p99'),\n",
    "    ('upa_max', 'PFDI_max')\n",
    "]\n",
    "\n",
    "for upa_col, pfdi_col in metrics_to_sync:\n",
    "    if upa_col in chronicle_events.columns:\n",
    "        chronicle_events[pfdi_col] = np.where(\n",
    "            chronicle_events['area_km2'] > 0,\n",
    "            chronicle_events[upa_col] / chronicle_events['area_km2'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "# Remove redundant columns to keep the dataset lean\n",
    "redundant_cols = ['polygon_total_area_m2', 'poly_area_km2', 'Unnamed: 0', 'version']\n",
    "chronicle_events = chronicle_events.drop(columns=[c for c in redundant_cols if c in chronicle_events.columns])\n",
    "\n",
    "# =================================================================\n",
    "# ACTION 2: INTENSITY ANALYSIS (Spatial then Temporal)\n",
    "# =================================================================\n",
    "print(f\"Step 2: Calculating Peak Intensities for {len(chronicle_events)} events...\")\n",
    "\n",
    "event_intensity_list = []\n",
    "\n",
    "for idx, row in tqdm(chronicle_events.iterrows(), total=len(chronicle_events)):\n",
    "    rain_matrix = row['imerg_matrix']\n",
    "    polygon_mask = row['imerg_mask']\n",
    "    \n",
    "    if not isinstance(rain_matrix, np.ndarray) or rain_matrix.size == 0 or polygon_mask.sum() == 0:\n",
    "        continue\n",
    "\n",
    "    # Spatial average across the entire polygon mask\n",
    "    spatial_mean_series = np.nanmean(rain_matrix[:, polygon_mask == 1], axis=1)\n",
    "    hyetograph = pd.Series(spatial_mean_series)\n",
    "\n",
    "    peak_stats = {'event_id': row['event_id']}\n",
    "    \n",
    "    # Temporal rolling max for each duration\n",
    "    for duration_min in DURATIONS:\n",
    "        window_steps = int(duration_min / 30)\n",
    "        if len(hyetograph) >= window_steps:\n",
    "            peak_val = hyetograph.rolling(window=window_steps).mean().max()\n",
    "            peak_stats[f\"{duration_min}_max_rainfall_intens\"] = peak_val\n",
    "        else:\n",
    "            peak_stats[f\"{duration_min}_max_rainfall_intens\"] = np.nan\n",
    "            \n",
    "    event_intensity_list.append(peak_stats)\n",
    "\n",
    "# =================================================================\n",
    "# FINAL MERGE & SAVE\n",
    "# =================================================================\n",
    "intensity_summary = pd.DataFrame(event_intensity_list)\n",
    "final_master_dataset = chronicle_events.merge(intensity_summary, on='event_id', how='left')\n",
    "\n",
    "print(f\"\\nProcessing Complete. Total events: {len(final_master_dataset)}\")\n",
    "final_master_dataset.to_pickle(RAIN_MASTER_FILE_PATH)\n",
    "print(f\"SUCCESS! Clean master dataset saved to: {RAIN_MASTER_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82996293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
