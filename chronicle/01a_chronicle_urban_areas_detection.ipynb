{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4712c29c",
   "metadata": {},
   "source": [
    "# Chronicle Flood Database Intersection with GHSL Built Area dataset\n",
    "\n",
    "This notebook provides a comprehensive analysis of the Chronicle urban flood dataset, containing over 880,000 flood events worldwide from 2000-2025.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7d1c3",
   "metadata": {},
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "781036f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt, geometry\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "from rasterstats import zonal_stats\n",
    "import numpy as np\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import seaborn as sns\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import math\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# File Paths\n",
    "chronicle_ds_csv_path = r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\chronicle_preliminary_huji.csv\"\n",
    "ghs_urban_raster_path = r\"D:\\Development\\RESEARCH\\Raanana\\data\\esa_worldcover\\GHS_BUILT_S_E2020_GLOBE_R2023A_54009_100_V1_0.tif\"\n",
    "\n",
    "# Processing Settings\n",
    "# Set to an integer (e.g., 10000) for testing, or None for full dataset\n",
    "SAMPLE_SIZE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923698c",
   "metadata": {},
   "source": [
    "# Data Loading & Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc84fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAMPLE_SIZE:\n",
    "    chronicle_raw_df = pd.read_csv(chronicle_ds_csv_path, nrows=SAMPLE_SIZE)\n",
    "else:\n",
    "    chronicle_raw_df = pd.read_csv(chronicle_ds_csv_path)\n",
    "\n",
    "# 2. Parse Geometries\n",
    "print(\"Parsing WKT geometries...\")\n",
    "chronicle_raw_df['geometry'] = chronicle_raw_df['geometry_wkt'].apply(wkt.loads)\n",
    "\n",
    "# 3. Create GeoDataFrame\n",
    "chronicle_gdf = gpd.GeoDataFrame(chronicle_raw_df, geometry='geometry')\n",
    "chronicle_gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# 4. Reproject\n",
    "with rasterio.open(ghs_urban_raster_path) as src:\n",
    "    ghs_raster_crs = src.crs\n",
    "    ghs_nodata_value = src.nodata\n",
    "    print(f\"Raster CRS: {ghs_raster_crs}\")\n",
    "    \n",
    "    print(\"Reprojecting polygons...\")\n",
    "    chronicle_projected_gdf = chronicle_gdf.to_crs(ghs_raster_crs)\n",
    "\n",
    "# 5. Calculate Zonal Statistics (WITH PROGRESS TRACKING)\n",
    "print(\"Calculating zonal stats in chunks...\")\n",
    "\n",
    "# -- PROGRESS LOGIC START --\n",
    "total_rows = len(chronicle_projected_gdf)\n",
    "# We want 10 updates, so we divide total rows by 10\n",
    "chunk_size = math.ceil(total_rows / 10) \n",
    "all_stats = []\n",
    "\n",
    "# Iterate through the DataFrame in chunks\n",
    "for i in range(0, total_rows, chunk_size):\n",
    "    # Slice the dataframe\n",
    "    subset = chronicle_projected_gdf.iloc[i : i + chunk_size]\n",
    "    \n",
    "    # Run stats just for this chunk\n",
    "    chunk_stats = zonal_stats(\n",
    "        subset,\n",
    "        ghs_urban_raster_path,\n",
    "        stats=['sum'],\n",
    "        nodata=ghs_nodata_value\n",
    "    )\n",
    "    \n",
    "    # Accumulate results\n",
    "    all_stats.extend(chunk_stats)\n",
    "    \n",
    "    # Calculate and print progress\n",
    "    current_row = min(i + chunk_size, total_rows)\n",
    "    percent_complete = int((current_row / total_rows) * 100)\n",
    "    print(f\"{percent_complete}% complete... ({current_row}/{total_rows})\")\n",
    "\n",
    "# Convert accumulated list to DataFrame\n",
    "urban_stats_df = pd.DataFrame(all_stats)\n",
    "# -- PROGRESS LOGIC END --\n",
    "\n",
    "# 6. Calculate Urban Percentage\n",
    "print(\"Calculating percentages...\")\n",
    "chronicle_projected_gdf['urban_built_up_area_m2'] = urban_stats_df['sum'].fillna(0)\n",
    "chronicle_projected_gdf['polygon_total_area_m2'] = chronicle_projected_gdf.area\n",
    "\n",
    "chronicle_projected_gdf['urban_percentage'] = np.where(\n",
    "    chronicle_projected_gdf['polygon_total_area_m2'] > 0,\n",
    "    (chronicle_projected_gdf['urban_built_up_area_m2'] / chronicle_projected_gdf['polygon_total_area_m2']) * 100,\n",
    "    0\n",
    ")\n",
    "\n",
    "# 7. Final Clean DataFrame\n",
    "chronicle_urban_df = pd.DataFrame(chronicle_projected_gdf.drop(columns='geometry'))\n",
    "\n",
    "print(f\"--- Done! Processed {len(chronicle_urban_df)} records. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb276801",
   "metadata": {},
   "source": [
    "## Save as a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4baf97dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a pickle file at the specified path\n",
    "chronicle_urban_df.to_pickle(r\"D:\\Development\\RESEARCH\\urban_flood_database\\chronicle\\chronicle_urban_df.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
